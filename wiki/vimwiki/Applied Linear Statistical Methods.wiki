= Applied Linear Statistical Methods =
== UChicago STAT 34300, Autumn 2023 ==

{{$
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\RSS}{RSS}
\DeclareMathOperator{\Jac}{Jac}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Im}{Im}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmin}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\rank}{rank}

\let\temp\phi
\let\phi\varphi
\let\varphi\temp

\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\bra}[1]{\left[#1\right]}
\newcommand{\cbra}[1]{\left\{#1\right\}}

\newcommand{\mat}[1]{\begin{matrix}#1\end{matrix}}
\newcommand{\pmat}[1]{\pa{\mat{#1}}}
\newcommand{\bmat}[1]{\bra{\mat{#1}}}

\newcommand{\pfrac}[2]{\pa{\frac{#1}{#2}}}
\newcommand{\bfrac}[2]{\bra{\frac{#1}{#2}}}
\newcommand{\psfrac}[2]{\pa{\sfrac{#1}{#2}}}
\newcommand{\bsfrac}[2]{\bra{\sfrac{#1}{#2}}}
}}$

Whenever it is unclear, vectors are column vectors.

=== Simple Linear Regression ===

[[https://en.wikipedia.org/wiki/Simple_linear_regression|Wikipedia]]

The setup of the simpliest model is as follows.

Take $x_i \in \mathbb R$ as predictors (alternatively, independent variables, features, etc), and $y_i \in \mathbb R$ as responses (alternatively, outcomes, etc.). Then, we hopes that the response is approximately linear in the predictors, e.g.
{{$
  y_i = \beta_0 + \beta_1 x_i + \epsilon_i
}}$
which we split (semantically) into a systemic component and some error.

Our goal is then to minimize the residual sum of squares, e.g. 
{{$
  \RSS(\mathbf \beta) = n^{-1}\sum_{i=1}^n \epsilon_i^2 = n^{-1}\sum_{i=1}^n(y_i - \beta_0 - \beta_1 x_i)^2.
}}$

Do this however you want; it doesn't matter. You will arrive at
{{$
\begin{align*}
  \hat \beta_1 &= \frac{\sum_{i=1}^n(x_i - \bar x)(y_i - \bar y)}{\sum_{i=1}^n(x_i - \bar x)^2} \\
  \hat \beta_0 &= \hat \alpha - \hat \beta_1 \bar x.
\end{align*}
}}$

Now, none of the above has any particular randomness as defined. However, we can now impose (some of) the following assumptions:
- We have i.i.d. $(x_1, y_1), \dots, (x_n, y_n)$.
- We have independent $y_i \sim P_{y_i \mid x = x_i}$ given fixed $x_i$.
- *Linearity*: $E[y_i \mid x_i] = \beta_0 + \beta_1 x_i$, and thus our residuals obey $E[\epsilon_i \mid x_i] = 0$.
- *Homoskedasticity*: $\Var(y_i \mid x_i) = \sigma^2 > 0 \iff \Var(\epsilon_i \mid x_i) = \sigma^2 > 0$.

Now for example, if we assume linearity, we can compute that $\hat \beta_1 = \beta_1 + \widetilde \epsilon$ with
{{$
  \Var(\widetilde \epsilon) = \frac{\sum(x_i - \bar x)^2 \Var(\epsilon_i)}{\left(\sum(x_i - \bar x)^2 \right)^2} = \frac{\sigma^2}{\sum(x_i - \bar x)^2} = \sigma_{\bar x}^2
}}$
where the last equality only holds under homoskedasticity, and we call the last quantity the standard error.

In this case, we can compute that by the CLT the asymptotic distribution of $\hat \beta_{1}$ is $N(\beta_1, \sigma_{\bar x}^2)$. In particular, we can form a confidence interval by doing the usual stuff.

Testing proceeds the same way: set some null $H_0: \beta_1 = c$ and some alternative $H_A: \beta_1 \neq c$, and set 
{{$
  \delta() = \begin{cases}
    1 & \text{if } H_0 \text{ is rejected} \\
    0 & \text{otherwise} \\
  \end{cases}.
}}$

Now take a test statistic $T \sim F_0$ under the null, with $p = 1 - F_0(T)$; this immediately yields that under the null, $p$ is uniformly distributed under the null (as long as $F_0$ is continuous), basically by definition.
Then, we set some threshold for $\alpha$, the probability of a Type-I error, and set $\delta = 1 \iff p \leq \alpha$.

Even under misspecification, we can see that if our samples are i.i.d. distributed $x_1, \dots, x_n \sim X$ and $y_1, \dots, y_n \sim Y$,
{{$
  \hat \beta_1 = \frac{n^{-1} \sum_{i=1}^n (x_i - \bar x)(y_i - \bar x)}{n^{-1}\sum_{i=1}^n (x_i - \bar x)^2} \overset{D}{\to} \frac{\Cov(X, Y)}{\Var(X)} = \rho_{XY} \cdot \frac{\sigma_X}{\sigma_Y}
}}$
where $\rho_{XY}$ is the correlation and $\sigma_X, \sigma_Y$ are standard deviations.

=== Multiple Linear Regression ===

[[https://en.wikipedia.org/wiki/Linear_regression|Wikipedia]]

Now we take $x_i \in \R^p$ and $y_i \in \R$, and set our model to be 
{{$
  y_i = \sum_{j=1}^p\beta_j (x_i)_j + \epsilon_i.
}}$
There are many different features that one could use.
- Taking a feature to be constantly $1$ is equivalent to adding a constant in our regression.
- One can take features that are functions of predictors (e.g. polynomals).
- You can take $\max \{ x_i - c, 0 \}$ to set a cutoff for a feature.

Notationally, set $y = \bmat{y_1 & \dots & y_n}^T \in \R^n$, $X = \bmat{x_1 & \dots & x_n}^T \in \R^{n \times p}$ where each $x_i$ is considered as column vector, $\beta = \bmat{ \beta_1 & \dots & \beta_n }$ and $\epsilon = \bmat{ \epsilon_1 & \dots & \epsilon_n }^T$ so that our model reduces to
{{$
  y = X\beta + \epsilon.
}}$

Then, the OLS estimator is given by
{{$
  \hat \beta = \argmin\{ \RSS(\beta) = \| y - X \beta \|^2\}
}}$
which we can compute directly taking a derivative (so long as $n \geq p$, e.g. we have more observations than features):
{{$
  \nabla_\beta \RSS(\beta) = -2X^T(Y - X\beta) = 0 \implies X^TX\beta = X^Ty
}}$
and if $X$ is invertible, we get
{{$
  \hat \beta = (X^TX)^{-1}X^Ty.
}}$

Why might $X$ not be invertible? Some examples are
- duplicated features,
- unit conversions / indentical measurements,
- batch effects (e.g. one feature is if a patient was given an medication $A$, and another is if a patient was given it by technician $A'$, but it was exactly $A'$ who handed out $A$).

==== OLS as a Projection ====

From a geometric perspective, the OLS predictions are just the projections of $y$ into the image of $X$ as a linear map $\R^p \to \R^n$.

_Def_: $X_{\cdot, 1}, \dots, X_{\cdot p}$ are *orthogonal* if $\left\langle X_{\cdot j}, X_{\cdot k}\right\rangle = 0$ for all $j, k$, and *orthonormal* if they are all of length 1.

*Theorem*: Set $V \subset \R^n$ a linear subspace and $y \in \R^n$; there exists a unique vector
{{$
  \proj_V(y) = \argmin_{v \in V} \| y - v \|.
}}$
Furthermore, $y - \proj_V(y) \in V^{\perp}$, the perpendicular space to $V$ (or equivalently, $\left\langle y - \proj_V(y), v \right\rangle = 0$ for all $v \in V$).

There are a few more facts:
- $y \mapsto \proj_V(y)$ is a linear operator, and the corresponding matrix is called the projection matrix $P_V \in \R^{n \times n} = P_V$;
- $P_V$ is idempotent, e.g. $P_V^2 = P_V$;
- $P_V^T = P_V$;
- $P_{V^\perp} = I - P_V$, which gives a decomposition $y = P_Vy + P_{V^\perp} y$.
- $\rank(P_V) = \dim(V)$, and has a eigenvalue decomposition with eigenvalues $\{ 0, 1 \}$ in the obvious way.

*Corollary*: Set $P_X = P_{\Im(X)}, \hat y = P_X y = X \hat \beta$. This immediately yields that the "fit" $\hat y$ and the residuals $\hat \epsilon$ are unique, and
{{$
  \|y\|^2 = \| \hat y \|^2 + \| \hat \epsilon \|^2
}}$
since $\hat \epsilon = (I - P_{X})y$.

_Example_: 
- If $X_{k 1} = 1$ for all $k$ (that is, we include a constant in our regression), we immediately get that $\sum_{i=1}^m \epsilon_i = 0$. 
- If $X$ is full rank, then $P_X = X(X^TX)^{-1}X^T$.

==== Reducing to Simple Linear Regression ====

If one column $X_{\cdot j}$ is orthogonal to every other feature, then this reduces to simplre linear regressions: 
{{$
  \hat \beta_j = \frac{\left\langle X_{\cdot j}, y\right\rangle}{\| X_{\cdot j} \|^2}.
}}$

Using this, set 
- $Z_{\cdot 1} = X_{ \cdot 1}$,
- $Z_{\cdot n}$ to the residuals of $X_{\cdot n} \sim Z_{\cdot n-1}$, e.g.
{{$
  X_{\cdot n} - \sum_{i=1}^{n-1}\frac{\left\langle Z_{\cdot i}, X_{\cdot i}\right\rangle}{\| Z_{\cdot i}\|^2}Z_{\cdot i}.
}}$

Then, we see that
{{$
  \hat \beta_p = \frac{\left\langle Z_{\cdot p}, y \right\rangle}{\| Z_{\cdot p}^2\|},
}}$
so in general each coefficient depends on other variables, e.g.
{{$
  \hat \beta_j = \frac{\left\langle \text{residuals of } X_{\cdot j} \sim \sum_{k \neq j}X_{\cdot k}, y\right\rangle}{ \| \text{residuals of } X_{\cdot j} \sim \sum_{k \neq j}X_{\cdot k} \|^2}.
}}$

In particular, the above (which is clearly just doing Gram-Schmidt) gives us a $QR$-decomposition of $X$, where $Q = ZD$ is the orthonormalization of $Z$ and $R$ is the upper triangular matrix corresponding to Gram-Schmidt.

This is (for the most part) what software packages do in computing linear models: you set $y = Q\gamma + \epsilon$, and take $\hat \gamma = Q^Ty$; but since the fit is unique, we have that $Q \hat \gamma = X \hat \beta = QR\hat \beta \implies R\hat \beta = \hat \gamma$, which is numerically tractible.

==== OLS and SVD ====

_Def_: [[https://en.wikipedia.org/wiki/Singular_value_decomposition|Link]] The *SVD decomposition* of a real (resp. complex) matrix $X \in \R^{n \times p}$ (resp. $\C^{n \times p}$) is
{{$
  X = U \Sigma V^*
}}$
where $U, V$ are orthogonal (resp. unitary) and $\Sigma$ is diagonal. Alternatively, we may sometimes use the "skinny" SVD, where $U, \Sigma$ are just $n \times p$ and $p \times p$ matrices, essentially by dropping the zero rows of $\Sigma$.

SVD is related to eigenvalue decomposition by noting that
{{$
  X^*X = V\Sigma V^* U \Sigma V^* = V(\Sigma^* \Sigma)V^*
}}$
so that columns of $V$ are eigenvectors of $X^*X$, and similarly columns of $U$ are eigenvectors of $XX^*$.
