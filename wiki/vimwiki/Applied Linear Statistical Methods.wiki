= Applied Linear Statistical Methods =
== UChicago STAT 34300, Autumn 2023 ==

{{$
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\RSS}{RSS}
\DeclareMathOperator{\Jac}{Jac}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Im}{Im}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmin}

\let\temp\phi
\let\phi\varphi
\let\varphi\temp

\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\bra}[1]{\left[#1\right]}
\newcommand{\cbra}[1]{\left\{#1\right\}}

\newcommand{\mat}[1]{\begin{matrix}#1\end{matrix}}
\newcommand{\pmat}[1]{\pa{\mat{#1}}}
\newcommand{\bmat}[1]{\bra{\mat{#1}}}

\newcommand{\pfrac}[2]{\pa{\frac{#1}{#2}}}
\newcommand{\bfrac}[2]{\bra{\frac{#1}{#2}}}
\newcommand{\psfrac}[2]{\pa{\sfrac{#1}{#2}}}
\newcommand{\bsfrac}[2]{\bra{\sfrac{#1}{#2}}}
}}$

Whenever it is unclear, vectors are column vectors.

=== Simple Linear Regression ===

[[https://en.wikipedia.org/wiki/Simple_linear_regression|Wikipedia Link]].

The setup of the simpliest model is as follows.

Take $x_i \in \mathbb R$ as predictors (alternatively, independent variables, features, etc), and $y_i \in \mathbb R$ as responses (alternatively, outcomes, etc.). Then, we hopes that the response is approximately linear in the predictors, e.g.
{{$
  y_i = \beta_0 + \beta_1 x_i + \epsilon_i
}}$
which we split (semantically) into a systemic component and some error.

Our goal is then to minimize the residual sum of squares, e.g. 
{{$
  \RSS(\mathbf \beta) = n^{-1}\sum_{i=1}^n \epsilon_i^2 = n^{-1}\sum_{i=1}^n(y_i - \beta_0 - \beta_1 x_i)^2.
}}$

Do this however you want; it doesn't matter. You will arrive at
{{$
\begin{align*}
  \hat \beta_1 &= \frac{\sum_{i=1}^n(x_i - \bar x)(y_i - \bar y)}{\sum_{i=1}^n(x_i - \bar x)^2} \\
  \hat \beta_0 &= \hat \alpha - \hat \beta_1 \bar x.
\end{align*}
}}$

Now, none of the above has any particular randomness as defined. However, we can now impose (some of) the following assumptions:
- We have i.i.d. $(x_1, y_1), \dots, (x_n, y_n)$.
- We have independent $y_i \sim P_{y_i \mid x = x_i}$ given fixed $x_i$.
- *Linearity*: $E[y_i \mid x_i] = \beta_0 + \beta_1 x_i$, and thus our residuals obey $E[\epsilon_i \mid x_i] = 0$.
- *Homoskedasticity*: $\Var(y_i \mid x_i) = \sigma^2 > 0 \iff \Var(\epsilon_i \mid x_i) = \sigma^2 > 0$.

Now for example, if we assume linearity, we can compute that $\hat \beta_1 = \beta_1 + \widetilde \epsilon$ with
{{$
  \Var(\widetilde \epsilon) = \frac{\sum(x_i - \bar x)^2 \Var(\epsilon_i)}{\left(\sum(x_i - \bar x)^2 \right)^2} = \frac{\sigma^2}{\sum(x_i - \bar x)^2} = \sigma_{\bar x}^2
}}$
where the last equality only holds under homoskedasticity, and we call the last quantity the standard error.

In this case, we can compute that by the CLT the asymptotic distribution of $\hat \beta_{1}$ is $N(\beta_1, \sigma_{\bar x}^2)$. In particular, we can form a confidence interval by doing the usual stuff.

Testing proceeds the same way: set some null $H_0: \beta_1 = c$ and some alternative $H_A: \beta_1 \neq c$, and set 
{{$
  \delta() = \begin{cases}
    1 & \text{if } H_0 \text{ is rejected} \\
    0 & \text{otherwise} \\
  \end{cases}.
}}$

Now take a test statistic $T \sim F_0$ under the null, with $p = 1 - F_0(T)$; this immediately yields that under the null, $p$ is uniformly distributed under the null (as long as $F_0$ is continuous), basically by definition.
Then, we set some threshold for $\alpha$, the probability of a Type-I error, and set $\delta = 1 \iff p \leq \alpha$.

Even under misspecification, we can see that if our samples are i.i.d. distributed $x_1, \dots, x_n \sim X$ and $y_1, \dots, y_n \sim Y$,
{{$
  \hat \beta_1 = \frac{n^{-1} \sum_{i=1}^n (x_i - \bar x)(y_i - \bar x)}{n^{-1}\sum_{i=1}^n (x_i - \bar x)^2} \overset{D}{\to} \frac{\Cov(X, Y)}{\Var(X)} = \rho_{XY} \cdot \frac{\sigma_X}{\sigma_Y}
}}$
where $\rho_{XY}$ is the correlation and $\sigma_X, \sigma_Y$ are standard deviations.

==== Multiple Linear Regression ====

[[https://en.wikipedia.org/wiki/Linear_regression|Wikipedia Link]].

Now we take $x_i \in \R^p$ and $y_i \in \R$, and set our model to be 
{{$
  y_i = \sum_{j=1}^p\beta_j (x_i)_j + \epsilon_i.
}}$
There are many different features that one could use.
- Taking a feature to be constantly $1$ is equivalent to adding a constant in our regression.
- One can take features that are functions of predictors (e.g. polynomals).
- You can take $\max \{ x_i - c, 0 \}$ to set a cutoff for a feature.

Notationally, set $y = \bmat{y_1 & \dots & y_n}^T \in \R^n$, $X = \bmat{x_1 & \dots & x_n}^T \in \R^{n \times p}$ where each $x_i$ is considered as column vector, $\beta = \bmat{ \beta_1 & \dots & \beta_n }$ and $\epsilon = \bmat{ \epsilon_1 & \dots & \epsilon_n }^T$ so that our model reduces to
{{$
  y = X\beta + \epsilon.
}}$

Then, the OLS estimator is given by
{{$
  \hat \beta = \argmin\{ \RSS(\beta) = \| y - X \beta \|^2\}
}}$
which we can compute directly taking a derivative (so long as $n \geq p$, e.g. we have more observations than features):
{{$
  \nabla_\beta \RSS(\beta) = -2X^T(Y - X\beta) = 0 \implies X^TX\beta = X^Ty
}}$
and if $X$ is invertible, we get
{{$
  \hat \beta = (X^TX)^{-1}X^Ty.
}}$

Why might $X$ not be invertible? Some examples are
- duplicated features,
- unit conversions / indentical measurements,
- batch effects (e.g. one feature is if a patient was given an medication $A$, and another is if a patient was given it by technician $A'$, but it was exactly $A'$ who handed out $A$).

From a geometric perspective, the OLS predictions are just the projections of $y$ into the image of $X$ as a linear map $\R^p \to \R^n$.
