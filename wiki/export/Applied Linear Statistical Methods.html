<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Applied Linear Statistical Methods</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="wiki.css" />
  <link rel="stylesheet" href="/wiki.css" />
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Applied Linear Statistical Methods</h1>
<h2 class="subtitle">UChicago STAT 34300, Autumn 2023</h2>
</header>
<hr>
<nav id="TOC" role="doc-toc">
<ul class="incremental">
<li><a href="#simple-linear-regression"
id="toc-simple-linear-regression">Simple Linear Regression</a></li>
<li><a href="#multiple-linear-regression"
id="toc-multiple-linear-regression">Multiple Linear Regression</a></li>
<li><a href="#distributions"
id="toc-distributions">Distributions</a></li>
<li><a href="#inference-in-the-homoskedastic-normal-linear-model"
id="toc-inference-in-the-homoskedastic-normal-linear-model">Inference in
the Homoskedastic Normal Linear Model</a></li>
</ul>
</nav>
<hr>
<p>Whenever it is unclear, vectors are column vectors.</p>
<h3 id="simple-linear-regression">Simple Linear Regression</h3>
<hr />
<p><a
href="https://en.wikipedia.org/wiki/Simple_linear_regression">Wikipedia</a></p>
<p>The setup of the simpliest model is as follows.</p>
<p>Take <span class="math inline">x_i \in \mathbb R</span> as predictors
(alternatively, independent variables, features, etc), and <span
class="math inline">y_i \in \mathbb R</span> as responses
(alternatively, outcomes, etc.). Then, we hopes that the response is
approximately linear in the predictors, e.g. <span class="math display">
  y_i = \beta_0 + \beta_1 x_i + \epsilon_i
</span> which we split (semantically) into a systemic component and some
error.</p>
<p>Our goal is then to minimize the residual sum of squares, e.g.  <span
class="math display">
  \mathop{\mathrm{RSS}}(\mathbf \beta) = n^{-1}\sum_{i=1}^n \epsilon_i^2
= n^{-1}\sum_{i=1}^n(y_i - \beta_0 - \beta_1 x_i)^2.
</span></p>
<p>Do this however you want; it doesn’t matter. You will arrive at <span
class="math display">
\begin{align*}
  \hat \beta_1 &amp;= \frac{\sum_{i=1}^n(x_i - \bar x)(y_i - \bar
y)}{\sum_{i=1}^n(x_i - \bar x)^2} \\
  \hat \beta_0 &amp;= \bar y  - \hat \beta_1 \bar x.
\end{align*}
</span></p>
<p>Now, none of the above has any particular randomness as defined.
However, we can now impose (some of) the following assumptions:</p>
<ul class="incremental">
<li>We have i.i.d. <span class="math inline">(x_1, y_1), \dots, (x_n,
y_n)</span>.</li>
<li>We have independent <span class="math inline">y_i \sim P_{y_i \mid x
= x_i}</span> given fixed <span class="math inline">x_i</span>.</li>
<li><strong>Linearity</strong>: <span class="math inline">E[y_i \mid
x_i] = \beta_0 + \beta_1 x_i</span>, and thus our residuals obey <span
class="math inline">E[\epsilon_i \mid x_i] = 0</span>.</li>
<li><strong>Homoskedasticity</strong>: <span
class="math inline">\mathop{\mathrm{Var}}(y_i \mid x_i) = \sigma^2 &gt;
0 \iff \mathop{\mathrm{Var}}(\epsilon_i \mid x_i) = \sigma^2 &gt;
0</span>.</li>
</ul>
<p>Now for example, if we assume linearity, we can compute that <span
class="math inline">\hat \beta_1 = \beta_1 + \widetilde \epsilon</span>
with <span class="math display">
  \mathop{\mathrm{Var}}(\widetilde \epsilon) = \frac{\sum(x_i - \bar
x)^2 \mathop{\mathrm{Var}}(\epsilon_i)}{\left(\sum(x_i - \bar x)^2
\right)^2} = \frac{\sigma^2}{\sum(x_i - \bar x)^2} = \sigma_{\bar x}^2
</span> where the last equality only holds under homoskedasticity, and
we call the last quantity the standard error.</p>
<p>In this case, we can compute that by the CLT the asymptotic
distribution of <span class="math inline">\hat \beta_{1}</span> is <span
class="math inline">N(\beta_1, \sigma_{\bar x}^2)</span>. In particular,
we can form a confidence interval by doing the usual stuff.</p>
<p>Testing proceeds the same way: set some null <span
class="math inline">H_0: \beta_1 = c</span> and some alternative <span
class="math inline">H_A: \beta_1 \neq c</span>, and set <span
class="math display">
  \delta = \begin{cases}
    1 &amp; \text{if } H_0 \text{ is rejected} \\
    0 &amp; \text{otherwise} \\
  \end{cases}.
</span></p>
<p>Now take a test statistic <span class="math inline">T \sim F_0</span>
under the null, with <span class="math inline">p = 1 - F_0(T)</span>;
this immediately yields that under the null, <span
class="math inline">p</span> is uniformly distributed under the null (as
long as <span class="math inline">F_0</span> is continuous), basically
by definition. Then, we set some threshold for <span
class="math inline">\alpha</span>, the probability of a Type-I error,
and set <span class="math inline">\delta = 1 \iff p \leq
\alpha</span>.</p>
<p>Even under misspecification, we can see that if our samples are
i.i.d. distributed <span class="math inline">x_1, \dots, x_n \sim
X</span> and <span class="math inline">y_1, \dots, y_n \sim Y</span>,
<span class="math display">
  \hat \beta_1 = \frac{n^{-1} \sum_{i=1}^n (x_i - \bar x)(y_i - \bar
y)}{n^{-1}\sum_{i=1}^n (x_i - \bar x)^2} \overset{D}{\to}
\frac{\mathop{\mathrm{Cov}}(X, Y)}{\mathop{\mathrm{Var}}(X)} = \rho_{XY}
\cdot \frac{\sigma_X}{\sigma_Y}
</span> where <span class="math inline">\rho_{XY}</span> is the
correlation and <span class="math inline">\sigma_X, \sigma_Y</span> are
standard deviations.</p>
<h3 id="multiple-linear-regression">Multiple Linear Regression</h3>
<hr />
<p><a
href="https://en.wikipedia.org/wiki/Linear_regression">Wikipedia</a></p>
<p>Now we take <span class="math inline">x_i \in \mathbb{R}^p</span> and
<span class="math inline">y_i \in \mathbb{R}</span>, and set our model
to be <span class="math display">
  y_i = \sum_{j=1}^p\beta_j (x_i)_j + \epsilon_i.
</span> There are many different features that one could use.</p>
<ul class="incremental">
<li>Taking a feature to be constantly <span class="math inline">1</span>
is equivalent to adding a constant in our regression.</li>
<li>One can take features that are functions of predictors
(e.g. polynomals).</li>
<li>You can take <span class="math inline">\max \{ x_i - c, 0 \}</span>
to set a cutoff for a feature.</li>
</ul>
<p>Notationally, set <span class="math inline">y =
\left[\begin{matrix}y_1 &amp; \dots &amp; y_n\end{matrix}\right]^T \in
\mathbb{R}^n</span>, <span class="math inline">X =
\left[\begin{matrix}x_1 &amp; \dots &amp; x_n\end{matrix}\right]^T \in
\mathbb{R}^{n \times p}</span> where each <span
class="math inline">x_i</span> is considered as column vector, <span
class="math inline">\beta = \left[\begin{matrix} \beta_1 &amp; \dots
&amp; \beta_n \end{matrix}\right]</span> and <span
class="math inline">\epsilon = \left[\begin{matrix} \epsilon_1 &amp;
\dots &amp; \epsilon_n \end{matrix}\right]^T</span> so that our model
reduces to <span class="math display">
  y = X\beta + \epsilon.
</span></p>
<p>Then, the OLS estimator is given by <span class="math display">
  \hat \beta = \mathop{\mathrm{argmin}}\{ \mathop{\mathrm{RSS}}(\beta) =
\| y - X \beta \|^2\}
</span> which we can compute directly taking a derivative (so long as
<span class="math inline">n \geq p</span>, e.g. we have more
observations than features): <span class="math display">
  \nabla_\beta \mathop{\mathrm{RSS}}(\beta) = -2X^T(Y - X\beta) = 0
\implies X^TX\beta = X^Ty
</span> and if <span class="math inline">X</span> is invertible, we get
<span class="math display">
  \hat \beta = (X^TX)^{-1}X^Ty.
</span></p>
<p>Why might <span class="math inline">X</span> not be invertible? Some
examples are</p>
<ul class="incremental">
<li>duplicated features,</li>
<li>unit conversions / indentical measurements,</li>
<li>batch effects (e.g. one feature is if a patient was given an
medication <span class="math inline">A</span>, and another is if a
patient was given it by technician <span
class="math inline">A&#39;</span>, but it was exactly <span
class="math inline">A&#39;</span> who handed out <span
class="math inline">A</span>).</li>
</ul>
<h4 id="ols-as-a-projection">OLS as a Projection</h4>
<p>From a geometric perspective, the OLS predictions are just the
projections of <span class="math inline">y</span> into the image of
<span class="math inline">X</span> as a linear map <span
class="math inline">\mathbb{R}^p \to \mathbb{R}^n</span>.</p>
<p><em>Def</em>: <span class="math inline">X_{\cdot, 1}, \dots, X_{\cdot
p}</span> are <strong>orthogonal</strong> if <span
class="math inline">\left\langle X_{\cdot j}, X_{\cdot k}\right\rangle =
0</span> for all <span class="math inline">j, k</span>, and
<strong>orthonormal</strong> if they are all of length 1.</p>
<p><strong>Theorem</strong>: Set <span class="math inline">V \subset
\mathbb{R}^n</span> a linear subspace and <span class="math inline">y
\in \mathbb{R}^n</span>; there exists a unique vector <span
class="math display">
  \mathop{\mathrm{proj}}_V(y) = \mathop{\mathrm{argmin}}_{v \in V} \| y
- v \|.
</span> Furthermore, <span class="math inline">y -
\mathop{\mathrm{proj}}_V(y) \in V^{\perp}</span>, the perpendicular
space to <span class="math inline">V</span> (or equivalently, <span
class="math inline">\left\langle y - \mathop{\mathrm{proj}}_V(y), v
\right\rangle = 0</span> for all <span class="math inline">v \in
V</span>).</p>
<p>There are a few more facts:</p>
<ul class="incremental">
<li><span class="math inline">y \mapsto
\mathop{\mathrm{proj}}_V(y)</span> is a linear operator, and the
corresponding matrix is called the projection matrix <span
class="math inline">P_V \in \mathbb{R}^{n \times n} = P_V</span>;</li>
<li><span class="math inline">P_V</span> is idempotent, e.g. <span
class="math inline">P_V^2 = P_V</span>;</li>
<li><span class="math inline">P_V^T = P_V</span>;</li>
<li><span class="math inline">P_{V^\perp} = I - P_V</span>, which gives
a decomposition <span class="math inline">y = P_Vy + P_{V^\perp}
y</span>.</li>
<li><span class="math inline">\mathop{\mathrm{rank}}(P_V) =
\dim(V)</span>, and has a eigenvalue decomposition with eigenvalues
<span class="math inline">\{ 0, 1 \}</span> in the obvious way.</li>
</ul>
<p><strong>Corollary</strong>: Set <span class="math inline">P_X =
P_{\mathop{\mathrm{Im}}(X)}, \hat y = P_X y = X \hat \beta</span>. This
immediately yields that the “fit” <span class="math inline">\hat
y</span> and the residuals <span class="math inline">\hat
\epsilon</span> are unique, and <span class="math display">
  \|y\|^2 = \| \hat y \|^2 + \| \hat \epsilon \|^2
</span> since <span class="math inline">\hat \epsilon = (I -
P_{X})y</span>.</p>
<p><em>Example</em>:</p>
<ul class="incremental">
<li>If <span class="math inline">X_{k 1} = 1</span> for all <span
class="math inline">k</span> (that is, we include a constant in our
regression), we immediately get that <span
class="math inline">\sum_{i=1}^m \epsilon_i = 0</span>.</li>
<li>If <span class="math inline">X</span> is full rank, then <span
class="math inline">P_X = X(X^TX)^{-1}X^T</span>.</li>
</ul>
<h4 id="reducing-to-simple-linear-regression">Reducing to Simple Linear
Regression</h4>
<p>If one column <span class="math inline">X_{\cdot j}</span> is
orthogonal to every other feature, then this reduces to simplre linear
regressions: <span class="math display">
  \hat \beta_j = \frac{\left\langle X_{\cdot j}, y\right\rangle}{\|
X_{\cdot j} \|^2}.
</span></p>
<p>Using this, set</p>
<ul class="incremental">
<li><span class="math inline">Z_{\cdot 1} = X_{ \cdot 1}</span>,</li>
<li><span class="math inline">Z_{\cdot n}</span> to the residuals of
<span class="math inline">X_{\cdot n} \sim Z_{\cdot n-1}</span>, e.g.
<span class="math display">
X_{\cdot n} - \sum_{i=1}^{n-1}\frac{\left\langle Z_{\cdot i}, X_{\cdot
i}\right\rangle}{\| Z_{\cdot i}\|^2}Z_{\cdot i}.
</span></li>
</ul>
<p>Then, we see that <span class="math display">
  \hat \beta_p = \frac{\left\langle Z_{\cdot p}, y \right\rangle}{\|
Z_{\cdot p}^2\|},
</span> so in general each coefficient depends on other variables, e.g.
<span class="math display">
  \hat \beta_j = \frac{\left\langle \text{residuals of } X_{\cdot j}
\sim \sum_{k \neq j}X_{\cdot k}, y\right\rangle}{ \| \text{residuals of
} X_{\cdot j} \sim \sum_{k \neq j}X_{\cdot k} \|^2}.
</span></p>
<p>In particular, the above (which is clearly just doing Gram-Schmidt)
gives us a <span class="math inline">QR</span>-decomposition of <span
class="math inline">X</span>, where <span class="math inline">Q =
ZD</span> is the orthonormalization of <span
class="math inline">Z</span> and <span class="math inline">R</span> is
the upper triangular matrix corresponding to Gram-Schmidt.</p>
<p>This is (for the most part) what software packages do in computing
linear models: you set <span class="math inline">y = Q\gamma +
\epsilon</span>, and take <span class="math inline">\hat \gamma =
Q^Ty</span>; but since the fit is unique, we have that <span
class="math inline">Q \hat \gamma = X \hat \beta = QR\hat \beta \implies
R\hat \beta = \hat \gamma</span>, which is numerically tractible.</p>
<h4 id="ols-and-svd">OLS and SVD</h4>
<p><em>Def</em>: <a
href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Link</a>
The <strong>SVD decomposition</strong> of a real (resp. complex) matrix
<span class="math inline">X \in \mathbb{R}^{n \times p}</span> (resp.
<span class="math inline">\mathbb{C}^{n \times p}</span>) is <span
class="math display">
  X = U \Sigma V^*
</span> where <span class="math inline">U, V</span> are orthogonal
(resp. unitary) and <span class="math inline">\Sigma</span> is diagonal.
Alternatively, we may sometimes use the “skinny” SVD, where <span
class="math inline">U, \Sigma</span> are just <span
class="math inline">n \times p</span> and <span class="math inline">p
\times p</span> matrices, essentially by dropping the zero rows of <span
class="math inline">\Sigma</span>.</p>
<p>SVD is related to eigenvalue decomposition by noting that <span
class="math display">
  X^*X = V\Sigma V^* U \Sigma V^* = V(\Sigma^* \Sigma)V^*
</span> so that columns of <span class="math inline">V</span> are
eigenvectors of <span class="math inline">X^*X</span>, and similarly
columns of <span class="math inline">U</span> are eigenvectors of <span
class="math inline">XX^*</span>.</p>
<p>Then, applying to least squares, we have that <span
class="math display">
\begin{align*}
  \| y - X \beta \|^2 &amp;= \| y - U \Sigma V^T \beta \|^2  \\
  &amp;= \| U^Ty - \Sigma V^T B \| \\
  &amp;= \| U^Ty - \Sigma \beta^* \|^2 \\
  &amp;= \sum_{j=1}^p ((U^Ty)_j - \sigma_j \beta_j^*)^2 \\
  &amp;= \sum_{j=1}^k (U^Ty)_j - \sigma_j \beta_j^*)^2 + \sum_{j=k+1}^p
(U^Ty)_j)^2 \\
\end{align*}
</span></p>
<p>So minimizing with respect to <span
class="math inline">\beta_j^*</span>, we pick <span
class="math display">
  \hat \beta_j^* = \begin{cases}
    \frac{(U^Ty)_j}{\sigma_j} &amp;  j = 1, \dots, k \\
    \text{anything} &amp; j = k + 1, \dots, p
  \end{cases}
</span> and in particular if we take the arbitrary values to be 0 we get
the “minimal norm” solution and since <span class="math inline">\beta^*
= V^T \beta</span>, the “ridgeless” solution is <span
class="math inline">\hat \beta = V\beta^*</span>, and is <span
class="math display">
  \mathop{\mathrm{argmin}}\{ \| \beta \| \mid \beta \in \mathbb{R}^p,
X^TX\beta = X^Ty \}.
</span></p>
<h3 id="distributions">Distributions</h3>
<hr />
<p>We covered a bunch of stuff that you can probably find on
Wikipedia.</p>
<ul class="incremental">
<li><a
href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">Multivariate
Gaussian Distributions</a></li>
<li><a
href="https://en.wikipedia.org/wiki/Chi-squared_distribution">Chi-Square
Distribution</a></li>
<li><a
href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">Student-t
Distribution</a></li>
<li><a href="https://en.wikipedia.org/wiki/F-distribution">F
Distribution</a></li>
</ul>
<h3 id="inference-in-the-homoskedastic-normal-linear-model">Inference in
the Homoskedastic Normal Linear Model</h3>
<hr />
<p>For this section, set <span class="math inline">X \in \mathbb{R}^{n
\times p}</span> design matrix, which is taken to be deterministic and
of full rank. Then, set <span class="math inline">y \sim N(X \beta,
\sigma^2 I)</span> for some <span class="math inline">\sigma &gt; 0,
\beta \in \mathbb{R}^p</span>; that is, set <span class="math display">
  Y = X\beta + \epsilon, \epsilon \sim N(0, \sigma^2 I).
</span> We have already seen <span class="math display">
  \hat \beta = (X^TX)^{-1}X^TY
</span> and so <span class="math inline">\hat \beta</span> is normal
with mean <span class="math inline">\beta</span> and variance <span
class="math display">
  \mathop{\mathrm{Var}}(\hat \beta) = (X^TX)^{-1}X^T(\sigma^2
I)X(X^TX)^{-1} = \sigma^2 (X^TX)^{-1}.
</span></p>
<p>But now we need to understand <span
class="math inline">\sigma</span>: we may split <span
class="math inline">Y</span> into <span class="math inline">\hat Y =
P_XY</span> and <span class="math inline">\hat \epsilon = (I -
P_X)Y</span>, where <span class="math inline">P_X</span> is the
orthogonal projection matrix onto <span class="math inline">X</span>;
properties of the multivariate Gaussian imply that these two are
independent, and have distributions <span class="math display">
  \begin{align*}
    \hat Y &amp;\sim N(X \beta, \sigma^2 P_X) \\
    \hat \epsilon &amp;\sim N(0, \sigma^2(I - P_X))
  \end{align*}
</span></p>
<p>which we can now use <span class="math inline">\hat \epsilon</span>
to learn about <span class="math inline">\sigma^2</span> and <span
class="math inline">\hat Y</span> to learn about <span
class="math inline">\beta</span>.</p>
<p>Specifically, we use the residual sum of squares <span
class="math inline">RSS = \| \hat \epsilon \|^2 \sim \sigma^2
\chi^2_{n-p}</span>; thus this has expectation <span
class="math inline">\sigma^2(n - p)</span>, so we can set <span
class="math display">
  \hat \sigma^2 = \frac{\| \epsilon^2 \|}{n - p} \sim \frac{\sigma^2
\chi^2_{n - p}}{n - p}.
</span> Moreover, since <span class="math inline">X^T = X^TP_X</span>,
we already know that <span class="math inline">\hat \beta</span> only
depends on <span class="math inline">\hat y</span>, and so is
independent of <span class="math inline">\hat \sigma</span>.</p>
<h4 id="applications">Applications</h4>
<p>We can now do inference for <span class="math inline">\beta_j</span>;
in particular <span class="math inline">\hat \beta_j \sim N(\beta_j,
\sigma^2 (X^TX)_{jj}^{-1})</span>. Then we know that if we take <span
class="math display">
\begin{align*}
  A &amp;= \frac{\hat \beta_j -  \beta_j}{\sigma \sqrt{(X^TX)^{-1}_jj}}
\sim N(0, 1) \\
  B &amp;= \frac{\hat \sigma^2 }{\sigma^2} \sim \frac{\chi^2_{n-p}}{n -
p}
\end{align*}
</span> so <span class="math inline">A / \sqrt{B} \sim t_{n - p}</span>;
that is, <span class="math display">
  \frac{\hat \beta_j -  \beta_j}{\hat \sigma \sqrt{(X^TX)^{-1}_jj}} \sim
t_{n - p}
</span> so we can get a concrete handle on the sampling distributions
from only our observations.</p>
</body>
</html>
