<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/wiki.css">
  <title>Applied Linear Statistical Methods</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="content">
      
<div id="Applied Linear Statistical Methods"><h1 id="Applied Linear Statistical Methods" class="header"><a href="#Applied Linear Statistical Methods">Applied Linear Statistical Methods</a></h1></div>
<div id="Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023"><h2 id="UChicago STAT 34300, Autumn 2023" class="header"><a href="#Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023">UChicago STAT 34300, Autumn 2023</a></h2></div>

\[
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\RSS}{RSS}
\DeclareMathOperator{\Jac}{Jac}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Im}{Im}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmin}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\rank}{rank}

\let\temp\phi
\let\phi\varphi
\let\varphi\temp

\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\bra}[1]{\left[#1\right]}
\newcommand{\cbra}[1]{\left\{#1\right\}}

\newcommand{\mat}[1]{\begin{matrix}#1\end{matrix}}
\newcommand{\pmat}[1]{\pa{\mat{#1}}}
\newcommand{\bmat}[1]{\bra{\mat{#1}}}

\newcommand{\pfrac}[2]{\pa{\frac{#1}{#2}}}
\newcommand{\bfrac}[2]{\bra{\frac{#1}{#2}}}
\newcommand{\psfrac}[2]{\pa{\sfrac{#1}{#2}}}
\newcommand{\bsfrac}[2]{\bra{\sfrac{#1}{#2}}}
\]

<p>
Whenever it is unclear, vectors are column vectors.
</p>

<div id="Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023-Simple Linear Regression"><h3 id="Simple Linear Regression" class="header"><a href="#Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023-Simple Linear Regression">Simple Linear Regression</a></h3></div>

<p>
<a href="https://en.wikipedia.org/wiki/Simple_linear_regression">Wikipedia</a>
</p>

<p>
The setup of the simpliest model is as follows.
</p>

<p>
Take \(x_i \in \mathbb R\) as predictors (alternatively, independent variables, features, etc), and \(y_i \in \mathbb R\) as responses (alternatively, outcomes, etc.). Then, we hopes that the response is approximately linear in the predictors, e.g.
</p>
\[
  y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]
<p>
which we split (semantically) into a systemic component and some error.
</p>

<p>
Our goal is then to minimize the residual sum of squares, e.g. 
</p>
\[
  \RSS(\mathbf \beta) = n^{-1}\sum_{i=1}^n \epsilon_i^2 = n^{-1}\sum_{i=1}^n(y_i - \beta_0 - \beta_1 x_i)^2.
\]

<p>
Do this however you want; it doesn't matter. You will arrive at
</p>
\[
\begin{align*}
  \hat \beta_1 &amp;= \frac{\sum_{i=1}^n(x_i - \bar x)(y_i - \bar y)}{\sum_{i=1}^n(x_i - \bar x)^2} \\
  \hat \beta_0 &amp;= \hat \alpha - \hat \beta_1 \bar x.
\end{align*}
\]

<p>
Now, none of the above has any particular randomness as defined. However, we can now impose (some of) the following assumptions:
</p>
<ul>
<li>
We have i.i.d. \((x_1, y_1), \dots, (x_n, y_n)\).

<li>
We have independent \(y_i \sim P_{y_i \mid x = x_i}\) given fixed \(x_i\).

<li>
<span id="Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023-Simple Linear Regression-Linearity"></span><strong id="Linearity">Linearity</strong>: \(E[y_i \mid x_i] = \beta_0 + \beta_1 x_i\), and thus our residuals obey \(E[\epsilon_i \mid x_i] = 0\).

<li>
<span id="Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023-Simple Linear Regression-Homoskedasticity"></span><strong id="Homoskedasticity">Homoskedasticity</strong>: \(\Var(y_i \mid x_i) = \sigma^2 &gt; 0 \iff \Var(\epsilon_i \mid x_i) = \sigma^2 &gt; 0\).

</ul>
<p>
Now for example, if we assume linearity, we can compute that \(\hat \beta_1 = \beta_1 + \widetilde \epsilon\) with
</p>
\[
  \Var(\widetilde \epsilon) = \frac{\sum(x_i - \bar x)^2 \Var(\epsilon_i)}{\left(\sum(x_i - \bar x)^2 \right)^2} = \frac{\sigma^2}{\sum(x_i - \bar x)^2} = \sigma_{\bar x}^2
\]
<p>
where the last equality only holds under homoskedasticity, and we call the last quantity the standard error.
</p>

<p>
In this case, we can compute that by the CLT the asymptotic distribution of \(\hat \beta_{1}\) is \(N(\beta_1, \sigma_{\bar x}^2)\). In particular, we can form a confidence interval by doing the usual stuff.
</p>

<p>
Testing proceeds the same way: set some null \(H_0: \beta_1 = c\) and some alternative \(H_A: \beta_1 \neq c\), and set 
</p>
\[
  \delta() = \begin{cases}
    1 &amp; \text{if } H_0 \text{ is rejected} \\
    0 &amp; \text{otherwise} \\
  \end{cases}.
\]

<p>
Now take a test statistic \(T \sim F_0\) under the null, with \(p = 1 - F_0(T)\); this immediately yields that under the null, \(p\) is uniformly distributed under the null (as long as \(F_0\) is continuous), basically by definition.
Then, we set some threshold for \(\alpha\), the probability of a Type-I error, and set \(\delta = 1 \iff p \leq \alpha\).
</p>

<p>
Even under misspecification, we can see that if our samples are i.i.d. distributed \(x_1, \dots, x_n \sim X\) and \(y_1, \dots, y_n \sim Y\),
</p>
\[
  \hat \beta_1 = \frac{n^{-1} \sum_{i=1}^n (x_i - \bar x)(y_i - \bar x)}{n^{-1}\sum_{i=1}^n (x_i - \bar x)^2} \overset{D}{\to} \frac{\Cov(X, Y)}{\Var(X)} = \rho_{XY} \cdot \frac{\sigma_X}{\sigma_Y}
\]
<p>
where \(\rho_{XY}\) is the correlation and \(\sigma_X, \sigma_Y\) are standard deviations.
</p>

<div id="Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023-Multiple Linear Regression"><h3 id="Multiple Linear Regression" class="header"><a href="#Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023-Multiple Linear Regression">Multiple Linear Regression</a></h3></div>

<p>
<a href="https://en.wikipedia.org/wiki/Linear_regression">Wikipedia</a>
</p>

<p>
Now we take \(x_i \in \R^p\) and \(y_i \in \R\), and set our model to be 
</p>
\[
  y_i = \sum_{j=1}^p\beta_j (x_i)_j + \epsilon_i.
\]
<p>
There are many different features that one could use.
</p>
<ul>
<li>
Taking a feature to be constantly \(1\) is equivalent to adding a constant in our regression.

<li>
One can take features that are functions of predictors (e.g. polynomals).

<li>
You can take \(\max \{ x_i - c, 0 \}\) to set a cutoff for a feature.

</ul>
<p>
Notationally, set \(y = \bmat{y_1 &amp; \dots &amp; y_n}^T \in \R^n\), \(X = \bmat{x_1 &amp; \dots &amp; x_n}^T \in \R^{n \times p}\) where each \(x_i\) is considered as column vector, \(\beta = \bmat{ \beta_1 &amp; \dots &amp; \beta_n }\) and \(\epsilon = \bmat{ \epsilon_1 &amp; \dots &amp; \epsilon_n }^T\) so that our model reduces to
</p>
\[
  y = X\beta + \epsilon.
\]

<p>
Then, the OLS estimator is given by
</p>
\[
  \hat \beta = \argmin\{ \RSS(\beta) = \| y - X \beta \|^2\}
\]
<p>
which we can compute directly taking a derivative (so long as \(n \geq p\), e.g. we have more observations than features):
</p>
\[
  \nabla_\beta \RSS(\beta) = -2X^T(Y - X\beta) = 0 \implies X^TX\beta = X^Ty
\]
<p>
and if \(X\) is invertible, we get
</p>
\[
  \hat \beta = (X^TX)^{-1}X^Ty.
\]

<p>
Why might \(X\) not be invertible? Some examples are
</p>
<ul>
<li>
duplicated features,

<li>
unit conversions / indentical measurements,

<li>
batch effects (e.g. one feature is if a patient was given an medication \(A\), and another is if a patient was given it by technician \(A'\), but it was exactly \(A'\) who handed out \(A\)).

</ul>
<div id="Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023-Multiple Linear Regression-OLS as a Projection"><h4 id="OLS as a Projection" class="header"><a href="#Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023-Multiple Linear Regression-OLS as a Projection">OLS as a Projection</a></h4></div>

<p>
From a geometric perspective, the OLS predictions are just the projections of \(y\) into the image of \(X\) as a linear map \(\R^p \to \R^n\).
</p>

<p>
<em>Def</em>: \(X_{\cdot, 1}, \dots, X_{\cdot p}\) are <span id="Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023-Multiple Linear Regression-OLS as a Projection-orthogonal"></span><strong id="orthogonal">orthogonal</strong> if \(\left\langle X_{\cdot j}, X_{\cdot k}\right\rangle = 0\) for all \(j, k\), and <span id="Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023-Multiple Linear Regression-OLS as a Projection-orthonormal"></span><strong id="orthonormal">orthonormal</strong> if they are all of length 1.
</p>

<p>
<span id="Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023-Multiple Linear Regression-OLS as a Projection-Theorem"></span><strong id="Theorem">Theorem</strong>: Set \(V \subset \R^n\) a linear subspace and \(y \in \R^n\); there exists a unique vector
</p>
\[
  \proj_V(y) = \argmin_{v \in V} \| y - v \|.
\]
<p>
Furthermore, \(y - \proj_V(y) \in V^{\perp}\), the perpendicular space to \(V\) (or equivalently, \(\left\langle y - \proj_V(y), v \right\rangle = 0\) for all \(v \in V\)).
</p>

<p>
There are a few more facts:
</p>
<ul>
<li>
\(y \mapsto \proj_V(y)\) is a linear operator, and the corresponding matrix is called the projection matrix \(P_V \in \R^{n \times n} = P_V\);

<li>
\(P_V\) is idempotent, e.g. \(P_V^2 = P_V\);

<li>
\(P_V^T = P_V\);

<li>
\(P_{V^\perp} = I - P_V\), which gives a decomposition \(y = P_Vy + P_{V^\perp} y\).

<li>
\(\rank(P_V) = \dim(V)\), and has a eigenvalue decomposition with eigenvalues \(\{ 0, 1 \}\) in the obvious way.

</ul>
<p>
<span id="Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023-Multiple Linear Regression-OLS as a Projection-Corollary"></span><strong id="Corollary">Corollary</strong>: Set \(P_X = P_{\Im(X)}, \hat y = P_X y = X \hat \beta\). This immediately yields that the "fit" \(\hat y\) and the residuals \(\hat \epsilon\) are unique, and
</p>
\[
  \|y\|^2 = \| \hat y \|^2 + \| \hat \epsilon \|^2
\]
<p>
since \(\hat \epsilon = (I - P_{X})y\).
</p>

<p>
<em>Example</em>: 
</p>
<ul>
<li>
If \(X_{k 1} = 1\) for all \(k\) (that is, we include a constant in our regression), we immediately get that \(\sum_{i=1}^m \epsilon_i = 0\). 

<li>
If \(X\) is full rank, then \(P_X = X(X^TX)^{-1}X^T\).

</ul>
<div id="Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023-Multiple Linear Regression-Reducing to Simple Linear Regression"><h4 id="Reducing to Simple Linear Regression" class="header"><a href="#Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023-Multiple Linear Regression-Reducing to Simple Linear Regression">Reducing to Simple Linear Regression</a></h4></div>

<p>
If one column \(X_{\cdot j}\) is orthogonal to every other feature, then this reduces to simplre linear regressions: 
</p>
\[
  \hat \beta_j = \frac{\left\langle X_{\cdot j}, y\right\rangle}{\| X_{\cdot j} \|^2}.
\]

<p>
Using this, set 
</p>
<ul>
<li>
\(Z_{\cdot 1} = X_{ \cdot 1}\),

<li>
\(Z_{\cdot n}\) to the residuals of \(X_{\cdot n} \sim Z_{\cdot n-1}\), e.g.
\[
  X_{\cdot n} - \sum_{i=1}^{n-1}\frac{\left\langle Z_{\cdot i}, X_{\cdot i}\right\rangle}{\| Z_{\cdot i}\|^2}Z_{\cdot i}.
\]

</ul>
<p>
Then, we see that
</p>
\[
  \hat \beta_p = \frac{\left\langle Z_{\cdot p}, y \right\rangle}{\| Z_{\cdot p}^2\|},
\]
<p>
so in general each coefficient depends on other variables, e.g.
</p>
\[
  \hat \beta_j = \frac{\left\langle \text{residuals of } X_{\cdot j} \sim \sum_{k \neq j}X_{\cdot k}, y\right\rangle}{ \| \text{residuals of } X_{\cdot j} \sim \sum_{k \neq j}X_{\cdot k} \|^2}.
\]

<p>
In particular, the above (which is clearly just doing Gram-Schmidt) gives us a \(QR\)-decomposition of \(X\), where \(Q = ZD\) is the orthonormalization of \(Z\) and \(R\) is the upper triangular matrix corresponding to Gram-Schmidt.
</p>

<p>
This is (for the most part) what software packages do in computing linear models: you set \(y = Q\gamma + \epsilon\), and take \(\hat \gamma = Q^Ty\); but since the fit is unique, we have that \(Q \hat \gamma = X \hat \beta = QR\hat \beta \implies R\hat \beta = \hat \gamma\), which is numerically tractible.
</p>

<div id="Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023-Multiple Linear Regression-OLS and SVD"><h4 id="OLS and SVD" class="header"><a href="#Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023-Multiple Linear Regression-OLS and SVD">OLS and SVD</a></h4></div>

<p>
<em>Def</em>: <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Link</a> The <span id="Applied Linear Statistical Methods-UChicago STAT 34300, Autumn 2023-Multiple Linear Regression-OLS and SVD-SVD decomposition"></span><strong id="SVD decomposition">SVD decomposition</strong> of a real (resp. complex) matrix \(X \in \R^{n \times p}\) (resp. \(\C^{n \times p}\)) is
</p>
\[
  X = U \Sigma V^*
\]
<p>
where \(U, V\) are orthogonal (resp. unitary) and \(\Sigma\) is diagonal. Alternatively, we may sometimes use the "skinny" SVD, where \(U, \Sigma\) are just \(n \times p\) and \(p \times p\) matrices, essentially by dropping the zero rows of \(\Sigma\).
</p>

<p>
SVD is related to eigenvalue decomposition by noting that
</p>
\[
  X^*X = V\Sigma V^* U \Sigma V^* = V(\Sigma^* \Sigma)V^*
\]
<p>
so that columns of \(V\) are eigenvectors of \(X^*X\), and similarly columns of \(U\) are eigenvectors of \(XX^*\).
</p>

    </div>
    <div class="footer">
      <p><small>Page created on 2023-10-04</small></p>
    </div>
</body>
</html>
