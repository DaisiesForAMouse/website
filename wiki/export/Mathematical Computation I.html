<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/wiki.css">
  <title>Mathematical Computation I</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="content">
      
<div id="Mathematical Computation I: Matrix Computation Course"><h1 id="Mathematical Computation I: Matrix Computation Course" class="header"><a href="#Mathematical Computation I: Matrix Computation Course">Mathematical Computation I: Matrix Computation Course</a></h1></div>
<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023"><h2 id="UChicago STAT 30900, Autumn 2023" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023">UChicago STAT 30900, Autumn 2023</a></h2></div>

\[
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

\DeclareMathOperator{\Jac}{Jac}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Spec}{Spec}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\diag}{diag}

\let\temp\phi
\let\phi\varphi
\let\varphi\temp

\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\bra}[1]{\left[#1\right]}
\newcommand{\cbra}[1]{\left\{#1\right\}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\lrnorm}[1]{\left\|#1\right\|}

\newcommand{\mat}[1]{\begin{matrix}#1\end{matrix}}
\newcommand{\pmat}[1]{\pa{\mat{#1}}}
\newcommand{\bmat}[1]{\bra{\mat{#1}}}

\newcommand{\pfrac}[2]{\pa{\frac{#1}{#2}}}
\newcommand{\bfrac}[2]{\bra{\frac{#1}{#2}}}
\newcommand{\psfrac}[2]{\pa{\sfrac{#1}{#2}}}
\newcommand{\bsfrac}[2]{\bra{\sfrac{#1}{#2}}}
\]

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review"><h3 id="Linear Algebra Review" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review">Linear Algebra Review</a></h3></div>

<p>
By convention, vectors are column vectors. 
</p>

<p>
Set \(V\) a vector space (almost always real or complex).
</p>

<p>
<em>Def</em>: \(\| \cdot \| : V \to \mathbb R\) is a <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-norm"></span><strong id="norm">norm</strong> if it satisfies the following properties:
</p>
<ul>
<li>
\(\|v\| \geq 0\) for any \(v \in V\),

<li>
\(\|v\| = 0\) iff \(v = 0\),

<li>
\(\|\alpha v\| = |\alpha| \|v\|\) for \(\alpha \in \mathbb R\) and \(v \in V\),

<li>
\( \|v + w\| \leq \|v\| + \|w\|\) for any \(v, w \in V\).

</ul>
<p>
Now, since this is a computational class, we only care about specific norms, almost all of which we can quickly qrite down. 
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms"><h4 id="Vector Norms" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms">Vector Norms</a></h4></div>

<p>
Set \(V = \mathbb R^n\) or \(\mathbb C^n\) equivalently.
</p>

<p>
<em>Def</em>: The <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms-Minkowski"></span><strong id="Minkowski">Minkowski</strong> or \(p\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms--norm"></span><strong id="-norm">-norm</strong> is given by
</p>
\[
  \| x \|_p = \left(\sum_{i=1} x_i^p\right)^{1/p}
\]
<p>
and we call the \(2\)-norm the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms-Euclidean norm"></span><strong id="Euclidean norm">Euclidean norm</strong> and the \(1\)-norm the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms-Manhattan norm"></span><strong id="Manhattan norm">Manhattan norm</strong>.
</p>

<p>
<em>Def</em>: The \(\infty\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms--norm"></span><strong id="-norm">-norm</strong> is the limit of \(p\)-norms as \(p \to \infty\), and is given by
</p>
\[
  \| x \|_\infty = \max_{i = 1, \dots, n} |x_i| = \lim_{p \to \infty} \| x \|_p.
\]

<p>
<em>Def</em>: For a weight vector \(\underline w = \begin{bmatrix}w_1, \dots, w_n\end{bmatrix}^T \in \mathbb R^n\), with each \(w_i &gt; 0\), we have that the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms-weighted"></span><strong id="weighted">weighted</strong> \(p\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms--norm"></span><strong id="-norm">-norm</strong> is
</p>
\[
  \| v \|_{\underline w, p} = \left(\sum_{i=1} w_i x_i^p\right)^{1/p}.
\]

<p>
<em>Def</em>: In general, for any positive definite matrix \(A\) (that is, \(x^TAx &gt; 0\) for all \(x \neq 0\)), we may consider the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms-Mahalanobis norm"></span><strong id="Mahalanobis norm">Mahalanobis norm</strong>
</p>
\[
  \| v \|_{A} = \left(x^T A x\right)^{1/2}.
\]

<p>
As convention, the "default" norm when a subscript is omitted is the Euclidean norm.
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms"><h4 id="Matrix Norms" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms">Matrix Norms</a></h4></div>

<p>
Set \(V = \mathbb R^{m \times n}\) or \(\mathbb C^{m \times n}\) equivalently.
</p>

<p>
<em>Def</em>: The <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms-Hölder"></span><strong id="Hölder">Hölder</strong> \(p\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms--norms"></span><strong id="-norms">-norms</strong> are given by
</p>
\[
  \| X \|_{H, p} = \left(\sum_{i=1}^m\sum_{j=1}^m |x_{ij}|^p \right)^{1/p},
\]
<p>
and the Hölder \(2\)-norm is called the Frobenius norm, which is also defined on infinite dimensional vector spaces as
</p>
\[
  \| X \|_F = \left(\Tr(XX^*)\right)^{1/2}
\]
<p>
where \(^*\) is the conjugate transpose.
</p>

<p>
<em>Def</em>: As before, we can take \(p \to \infty\) to get the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms-Hölder"></span><strong id="Hölder">Hölder</strong> \(\infty\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms--norm"></span><strong id="-norm">-norm</strong> given by
</p>
\[
  \| X\|_{H, \infty} = \max_{\substack{i = 1, \dots n \\ j = 1, \dots, n}} |x_{ij}|.
\]

<p>
<em>Def</em>: We can also define norms on matrices by viewing them as linear maps \(A: \mathbb R^n \to \mathbb R^m\); in particular, if we have some norm \(\| \cdot \|_a\) on \(\mathbb R^n\) and some norm \(\| \cdot \|_b\) on \(\mathbb R^m\), we may define the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms-operator norm"></span><strong id="operator norm">operator norm</strong> (or <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms-induced norm"></span><strong id="induced norm">induced norm</strong>)
</p>
\[
  \| A \|_{a, b} = \max_{x \neq 0} \frac{\| A x \|_b}{\| x \|_a}.
\]
<p>
In particular, if the norms on the domain and codomain are just \(p\)-norms, we write
</p>
\[
  \| A \|_{p} = \max_{x \neq 0}\frac{\| A x\|_p}{\| x \|_p}
\]
<p>
and call it the \(p\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms--norm"></span><strong id="-norm">-norm</strong> of \(A\). In particular, we call the \(2\)-norm the spectral norm. Further, the \(1\)-norm and \(\infty\)-norm are just
</p>
\[
  \| A \|_1 = \max_{j = 1, \dots, n} \left(\sum_{i=1}^m |a_{ij}| \right),
\]
<p>
which is the max column sum, and
</p>
\[
  \| A \|_\infty = \max_{i = 1, \dots, m} \left(\sum_{j=1}^n |a_{ij}| \right),
\]
<p>
which is the max row sum; both facts are easy to check.
</p>

<p>
In general, for \(p \notin \{1, 2, \infty\}\), computing \(\| A \|_p\) is NP-hard, and if we consider \(\| A \|_{p,q}\) then \(\|A\|_{\infty, 1}\) is hard and \(\|A\|_{1, \infty}\) is easy.
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms"><h4 id="Properties of Norms" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms">Properties of Norms</a></h4></div>

<p>
We may also want to consider some other desirable properties on our norms.
</p>
<ul>
<li>
For example, we might want <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms-submultiplicativity"></span><strong id="submultiplicativity">submultiplicativity</strong>:
\[
  \| A B\| \leq \|A \| \| B \|.
\]

</ul>
<p>
The Frobenius norm is submultiplicative.
</p>
<ul>
<li>
Take also <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms-consistency"></span><strong id="consistency">consistency</strong>:
\[
  \| Ax \|_b \leq \| A \|_{a,b} \|x\|_a.
\]

</ul>
<p>
This is true for \(p\)-norms, but not in general.
</p>

<p>
Some properties always hold.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms-Prop"></span><strong id="Prop">Prop</strong>: Every norm is Lipschitz.
</p>

<p>
<em>Proof</em>: Let our norm be \(\| \cdot \| : V \to \R\). The triangle inequality immediately implies
</p>
\[
  | \| u \| - \| v \| | \leq \| u - v \|.
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms-Theorem (Equivalence of Norms)"></span><strong id="Theorem (Equivalence of Norms)">Theorem (Equivalence of Norms)</strong>: <a href="https://kconrad.math.uconn.edu/blurbs/gradnumthy/equivnorms.pdf">Link</a> Set \(V\) a finite-dimensional vector space. Then every pair of norms \(\| \cdot \|_a, \| \cdot \|_b\) are equivalent to each other, e.g. there are constants \(c_1, c_2\) such that for any \(v \in V\),
</p>
\[
  c_1\| v \|_b \leq \| v \|_a \leq c_2 \| v \|_b.
\]

<p>
<em>Proof</em>: Induct on the dimension of \(V\) and see that every norm is equivalent to the infinity norm.
</p>

<p>
<em>Def</em>: We say that a sequence \(\{ x_k \}_{k=1}^\infty\) of vectors <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms-converges"></span><strong id="converges">converges</strong> to \(x\) if
</p>
\[
  \lim_{k \to \infty} \| x_k - x \| = 0.
\]

<p>
Then, the above clearly shows that convergence in one norm implies convergence in every norm.
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products"><h4 id="Inner, Outer, Matrix Products" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products">Inner, Outer, Matrix Products</a></h4></div>

<p>
<em>Def</em>: Set \(V\) a \(K\)-vector space (where \(K = \R, \C\)). An <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-inner product"></span><strong id="inner product">inner product</strong> is a binary operation \(\langle \cdot, \cdot \rangle: V \times V \to \R\) which satisfies that
</p>
<ul>
<li>
\(\left\langle v, v \right\rangle \geq 0\) for all \(v \in V\),

<li>
\(\left\langle v, v,\right\rangle = 0\) if and only if \(v = 0\),

<li>
\(\left\langle u, v \right\rangle = \overline{ \left\langle v, u\right\rangle }\) for all \(u, v \in V\),

<li>
\(\left\langle \alpha_1 u_1 + \alpha_2 u_2, v\right\rangle = \alpha \left\langle u_1, v \right\rangle + \alpha_2 \left\langle u_2, v\right\rangle\) for all \(u_1, u_2, v \in V, \alpha_1, \alpha_2 \in K\).

</ul>
<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Prop"></span><strong id="Prop">Prop</strong>: For an inner product \(\left\langle \cdot, \cdot \right\rangle\),
</p>
\[
  \| v \| = \sqrt{ \left\langle v, v \right\rangle }
\]
<p>
is a norm. Furthermore, an arbitrary norm \(\| \|\) is induced by an inner product if and only if it satisfies the parallelogram law
</p>
\[
  \norm{u + v}^2 + \norm{u - v}^2 = 2 \norm u ^ 2 + 2 \norm v ^2.
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Theorem (Cauchy-Schwarz)"></span><strong id="Theorem (Cauchy-Schwarz)">Theorem (Cauchy-Schwarz)</strong>: <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Link</a> Let \(\norm \cdot\) be induced by \(\left\langle \cdot, \cdot \right\rangle\). Then
</p>
\[
  \sqrt{\left\langle u, v \right\rangle} \leq \norm u \norm v.
\]


<p>
<em>Def</em>: The <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-standard Euclidean inner product"></span><strong id="standard Euclidean inner product">standard Euclidean inner product</strong> on \(\C^n\) is
</p>
\[
  \left\langle x, y\right\rangle = x^*y.
\]

<p>
<em>Def</em>: The <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Frobenius"></span><strong id="Frobenius">Frobenius</strong> norm on \(\C^{m \times n}\) is 
</p>
\[
  \left\langle X, Y\right\rangle = \sum_{i=1}^m \sum_{j=1}^n x_{ij} y_{ij} = \Tr(X^*Y).
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Theorem (Hölder Inequality)"></span><strong id="Theorem (Hölder Inequality)">Theorem (Hölder Inequality)</strong>: <a href="https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality">Link</a> For \(x, y \in \C^n\) and \(p^{-1} + q^{-1} = 1\), we have
</p>
\[
  |x^*y| \leq \norm x_p \norm y_q.
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Theorem (Bessel Inequality)"></span><strong id="Theorem (Bessel Inequality)">Theorem (Bessel Inequality)</strong>: <a href="https://en.wikipedia.org/wiki/Bessel%27s_inequality">Link</a> For \(x \in \C^n\) and an orthonormal basis \(e_1, \dots, e_n\), we have 
</p>
\[
  \sum_{k=1}^n | \left\langle x, e_k \right\rangle^2  \leq \norm x_2.
\]

<p>
<em>Def</em>: The <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-outer product"></span><strong id="outer product">outer product</strong> is a binary operator \(\C^m \times \C^n \to \C^{m \times n}\) taking 
</p>
\[
  (x, y) \mapsto xy^*.
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Prop"></span><strong id="Prop">Prop</strong>: \(A \in \R^{m \times n}\) is an outer product iff and only if it has rank 1.
</p>

<p>
<em>Def</em>: The matrix product is a binary operator \(\C^{m \times n} \times \C^{n \times p} \to \C^{M \times p}\). Set \(A = \bmat{\alpha_1 &amp; \alpha_2 &amp; \cdots &amp; \alpha_m}^T\) and \(B = \bmat{\beta_1 &amp; \beta_2 &amp; \cdots &amp; \beta_p}\). Then,
</p>
\[
  AB = \bmat{
    \alpha_1^T \beta_1 &amp; \cdots &amp; \alpha_1^T \beta_n \\
    \alpha_2^T \beta_1 &amp; \cdots &amp; \alpha_2^T \beta_n \\
    \vdots &amp; \ddots &amp; \vdots \\
    \alpha_m^T \beta_1 &amp; \cdots &amp; \alpha_m^T \beta_n 
  } 
  = \bmat{A\beta_1 &amp; A\beta_2 &amp; \cdots &amp; A\beta_n} 
  = \bmat{\alpha_1^TB \\ \alpha_2^TB \\ \vdots \\ \alpha_m^TB}.
\]
<p>
Alternatively, it is uniquely characterized as the matrix representing the composition of \(A\) and \(B\) as operators.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Prop"></span><strong id="Prop">Prop</strong>: Let \(D = \diag(d_1, \dots, d_n)\), the diagonal matrix with entries \(d_1, \dots, d_n\); then
</p>
\[
  AD = \bmat{d_1\alpha_1, \dots, d_n \alpha_n}.
\]
<p>
Simiar for the other direction of multiplication.
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors"><h4 id="Eigenvalues and Eigenvectors" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors">Eigenvalues and Eigenvectors</a></h4></div>

<p>
<em>Def</em>: For a complex matrix \(A\), an <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-eigenvalue"></span><strong id="eigenvalue">eigenvalue</strong> \(\lambda \in \C\) and <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-eigenvector"></span><strong id="eigenvector">eigenvector</strong> \(v \neq 0\) satisfy
</p>
\[
  Av = \lambda v.
\]

<p>
<em>Def</em>: Furthermore, an <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-eigenspace"></span><strong id="eigenspace">eigenspace</strong> is the span of all eigenvectors correspnding to a single eigenvalue, the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-spectrum"></span><strong id="spectrum">spectrum</strong> of \(A\), \(\Spec(A)\) is the set of all eigenvalues of \(A\), and the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-spectral radius"></span><strong id="spectral radius">spectral radius</strong> is \(\rho(A) = \max_{\lambda \in \Spec(A)} |\lambda| = |\lambda_{\text{max}}|\). Sometimes we will call this the top eigenvector/eigenvalue.
</p>

<p>
As a convention, usually we implcitly order eigenvalues, e.g. \(|\lambda_1| \geq |\lambda_2| \geq \cdots \geq \lambda_n\).
</p>

<p>
<em>Def</em>: More generally, if \(v\) is a eigenvector of \(A^T\), we say that \(v\) is a <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-left eigenvector"></span><strong id="left eigenvector">left eigenvector</strong> of \(A\) (and thus usual eigenvectors are <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-right eigenvectors"></span><strong id="right eigenvectors">right eigenvectors</strong>).
</p>

<p>
There are a few warnings about these things:
</p>
<ul>
<li>
real matrices may have complex eigenvectors;

<li>
we normally normalize eigenvectors to have unit length;

<li>
left and right eigenvectors are usually not the same.

</ul>
<p>
<em>Def</em>: A square matrix \(A \in \C^{n \times n}\) is diagonalizable if it is simiarly to a diagonal matrix.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-Prop"></span><strong id="Prop">Prop</strong>: \(A\) is diagonalizable if and only if it has \(n\) linearly independent eigenvectors.
</p>

<p>
<em>Proof</em>: The eigenvectors form a basis by intertibility; change basis from the eigenvectors, scale by eigenvalues, and change basis back. In particular, let \(A \in \C^{n \times n}\) have eigenvalues \(\lambda_1, \dots, \lambda_n\), with corresponding eigenvectors \(v_1, \dots, v_n\); set \(\Lambda = \diag(\lambda_1, \dots, \lambda_n)\) and \(V = \bmat{v_1, \dots, v_n}\). Then
</p>
\[
  A = V \Lambda V^{-1}.
\]

<p>
<em>Def</em>: The above \(A = X \Lambda X^{-1}\) is called the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-eigenvalue decomposition"></span><strong id="eigenvalue decomposition">eigenvalue decomposition</strong>, or <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-EVD"></span><strong id="EVD">EVD</strong>.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-Prop"></span><strong id="Prop">Prop</strong>: The diagonal entries in a triangular matrix are just the eigenvalues.
</p>

<p>
<em>Def</em>: A matrix \(A \in \C^{n \times n}\) is <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-normal"></span><strong id="normal">normal</strong> if it commutes with its adjoint \(A^*\) (the conjugate transpose), and <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-unitary"></span><strong id="unitary">unitary</strong> if \(AA^* = A^*A = I\).
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-Prop"></span><strong id="Prop">Prop</strong>: \(A\) is unitary if and only if it's columns (or rows) are orthonormal.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-Theorem"></span><strong id="Theorem">Theorem</strong>: \(A \in \C^{n \times n}\) is normal if and only if it is unitarily diagonalizable, e.g. if \(A = V\Lambda V^*\) with \(V\) unitary.
</p>

<p>
<em>Def</em>: \(A \in \C^{n \times n}\) is <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-Hermitian"></span><strong id="Hermitian">Hermitian</strong> if \(A^* = A\).
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-Theorem (Spectral Theorem)"></span><strong id="Theorem (Spectral Theorem)">Theorem (Spectral Theorem)</strong>: <a href="https://en.wikipedia.org/wiki/Spectral_theorem ">Link</a> \(A \in \C^{n \times n}\) is Hermetian if and only if \(A\) is unitarily diagonalizable with all real eigenvalues.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-Corollary"></span><strong id="Corollary">Corollary</strong>: \(A \in \R^{n \times n}\) is symmetric if and only if it is orthgonally diagonalizable with all eigenvalues real.
</p>

<p>
<em>Def</em>: <a href="https://en.wikipedia.org/wiki/Jordan_normal_form">Link</a> Any matrix can be written in <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-Jordan canonical form"></span><strong id="Jordan canonical form">Jordan canonical form</strong>, e.g.
</p>
\[
  A = XJX^{-1}
\]
<p>
where \(J\) is nonzero only on the diagonal and the superdiagonal, which has values only in \(\{0, 1\}\), such as
</p>
\[
  J = \begin{bmatrix}
    \lambda_1 &amp; 1         &amp;           &amp;   &amp; &amp; &amp; \\
              &amp; \lambda_1 &amp; 1         &amp;   &amp; &amp; &amp; \\
              &amp;           &amp; \lambda_1 &amp; 0 &amp; &amp; &amp; \\
              &amp;           &amp; &amp; \lambda_2 &amp; 0 &amp; &amp; \\
              &amp;           &amp;         &amp;  &amp; \lambda_3 &amp; 1 &amp; \\
              &amp;           &amp;         &amp;  &amp; &amp; \lambda_3  &amp; \\
  \end{bmatrix}
\]
<p>
for example.
</p>

<p>
The way it's written above is not by coincidence: you can permute everything so that \(J\) is composed of Jordan blocks, which have the same entry all the way down the diagonal and have a superdiagonal of only ones, e.g.
</p>
\[
  J = \begin{bmatrix}
    J_1 &amp; &amp; &amp; \\
    &amp; J_2 &amp; &amp; \\
    &amp; &amp; \ddots &amp; \\
    &amp; &amp; &amp; J_k \\
  \end{bmatrix}
\]
<p>
where each \(J_i = \lambda I + N\), where \(N\) has all zero entries except on the superdiagonal, on which it is always one.
</p>

<p>
Unfortunately, the JCF is pretty useless in application.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-Theorem (Golub-Wilkinson)"></span><strong id="Theorem (Golub-Wilkinson)">Theorem (Golub-Wilkinson)</strong>: The Jordan canonical form cannot be computed in finite precision.
</p>

<p>
Let's return to the spectral radius,
</p>
\[
  \rho(A) = \max_{\lambda \in \Spec(A)} |\lambda|.
\]

<p>
Any nilpotent matrix shows that \(\rho\) is not a norm, but the following is true.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-Prop"></span><strong id="Prop">Prop</strong>: If \(\| \cdot \|: \C^{m \times m} \to \R\) is any consistent matrix norm, then \(\rho(A) \leq \| A \|\).
</p>

<p>
<em>Proof</em>: Look at the norm of the image of a top eigenvector.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-Theorem"></span><strong id="Theorem">Theorem</strong>: Given any \(A \in \C^{n \times n}\), any positive \(\epsilon\), there is a consistent norm (in fact, an operator norm) \(\| \cdot \|: \C^{n \times m} \to \R\) such that 
</p>
\[
  \| A \| \leq \rho(A) + \epsilon.
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-Prop"></span><strong id="Prop">Prop</strong>: Given any matrix norm \(\| \cdot \|: \C^{n \times m} \to R\), we have
</p>
\[
  \rho(A) = \lim_{k \to \infty} \| A^k \|^{1/k}.
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Eigenvalues and Eigenvectors-Lemma"></span><strong id="Lemma">Lemma</strong>: \(\lim_{k \to \infty} A^k = 0\)  if and only if \(\rho(A) &lt; 1\).
</p>

<p>
<em>Proof</em>: \((\implies)\) Set \(\lambda\) to be a top eigenvalue of \(A\), and \(x\) a corresponding eigenvector. Then, \(A^k x = \lambda^k x\). Send \(k \to \infty\) and conclude.
</p>

<p>
\((\impliedby)\) By the above theorems, there is some operator norm \(\| \cdot \|\) such that \(\|A\| \leq \rho(A) + \epsilon &lt; 1\). Send \(k \to \infty\) and use the fact that operator norms imply \(\| A^k \| \leq \|A\|^k\) and win.
</p>

    </div>
    <div class="footer">
      <p><small>Page created on 2023-10-04</small></p>
    </div>
</body>
</html>
