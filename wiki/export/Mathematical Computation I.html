<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/wiki.css">
  <title>Mathematical Computation I</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="content">
      
<div id="Mathematical Computation I: Matrix Computation Course"><h1 id="Mathematical Computation I: Matrix Computation Course" class="header"><a href="#Mathematical Computation I: Matrix Computation Course">Mathematical Computation I: Matrix Computation Course</a></h1></div>
<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023"><h2 id="UChicago STAT 30900, Autumn 2023" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023">UChicago STAT 30900, Autumn 2023</a></h2></div>

\[
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

\DeclareMathOperator{\Jac}{Jac}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Spec}{Spec}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\argmin}{argmin}

\let\temp\phi
\let\phi\varphi
\let\varphi\temp

\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\bra}[1]{\left[#1\right]}
\newcommand{\cbra}[1]{\left\{#1\right\}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\lrnorm}[1]{\left\|#1\right\|}

\newcommand{\mat}[1]{\begin{matrix}#1\end{matrix}}
\newcommand{\pmat}[1]{\pa{\mat{#1}}}
\newcommand{\bmat}[1]{\bra{\mat{#1}}}

\newcommand{\pfrac}[2]{\pa{\frac{#1}{#2}}}
\newcommand{\bfrac}[2]{\bra{\frac{#1}{#2}}}
\newcommand{\psfrac}[2]{\pa{\sfrac{#1}{#2}}}
\newcommand{\bsfrac}[2]{\bra{\sfrac{#1}{#2}}}
\]

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review"><h3 id="Linear Algebra Review" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review">Linear Algebra Review</a></h3></div>

<p>
By convention, vectors are column vectors. 
</p>

<p>
Set \(V\) a vector space (almost always real or complex).
</p>

<p>
<em>Def</em>: \(\| \cdot \| : V \to \mathbb R\) is a <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-norm"></span><strong id="norm">norm</strong> if it satisfies the following properties:
</p>
<ul>
<li>
\(\|v\| \geq 0\) for any \(v \in V\),

<li>
\(\|v\| = 0\) iff \(v = 0\),

<li>
\(\|\alpha v\| = |\alpha| \|v\|\) for \(\alpha \in \mathbb R\) and \(v \in V\),

<li>
\( \|v + w\| \leq \|v\| + \|w\|\) for any \(v, w \in V\).

</ul>
<p>
Now, since this is a computational class, we only care about specific norms, almost all of which we can quickly qrite down. 
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms"><h4 id="Vector Norms" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms">Vector Norms</a></h4></div>

<p>
Set \(V = \mathbb R^n\) or \(\mathbb C^n\) equivalently.
</p>

<p>
<em>Def</em>: The <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms-Minkowski"></span><strong id="Minkowski">Minkowski</strong> or \(p\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms--norm"></span><strong id="-norm">-norm</strong> is given by
</p>
\[
  \| x \|_p = \left(\sum_{i=1} x_i^p\right)^{1/p}
\]
<p>
and we call the \(2\)-norm the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms-Euclidean norm"></span><strong id="Euclidean norm">Euclidean norm</strong> and the \(1\)-norm the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms-Manhattan norm"></span><strong id="Manhattan norm">Manhattan norm</strong>.
</p>

<p>
<em>Def</em>: The \(\infty\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms--norm"></span><strong id="-norm">-norm</strong> is the limit of \(p\)-norms as \(p \to \infty\), and is given by
</p>
\[
  \| x \|_\infty = \max_{i = 1, \dots, n} |x_i| = \lim_{p \to \infty} \| x \|_p.
\]

<p>
<em>Def</em>: For a weight vector \(\underline w = \begin{bmatrix}w_1, \dots, w_n\end{bmatrix}^T \in \mathbb R^n\), with each \(w_i &gt; 0\), we have that the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms-weighted"></span><strong id="weighted">weighted</strong> \(p\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms--norm"></span><strong id="-norm">-norm</strong> is
</p>
\[
  \| v \|_{\underline w, p} = \left(\sum_{i=1} w_i x_i^p\right)^{1/p}.
\]

<p>
<em>Def</em>: In general, for any positive definite matrix \(A\) (that is, \(x^TAx &gt; 0\) for all \(x \neq 0\)), we may consider the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms-Mahalanobis norm"></span><strong id="Mahalanobis norm">Mahalanobis norm</strong>
</p>
\[
  \| v \|_{A} = \left(x^T A x\right)^{1/2}.
\]

<p>
As convention, the "default" norm when a subscript is omitted is the Euclidean norm.
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms"><h4 id="Matrix Norms" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms">Matrix Norms</a></h4></div>

<p>
Set \(V = \mathbb R^{m \times n}\) or \(\mathbb C^{m \times n}\) equivalently.
</p>

<p>
<em>Def</em>: The <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms-Hölder"></span><strong id="Hölder">Hölder</strong> \(p\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms--norms"></span><strong id="-norms">-norms</strong> are given by
</p>
\[
  \| X \|_{H, p} = \left(\sum_{i=1}^m\sum_{j=1}^m |x_{ij}|^p \right)^{1/p},
\]
<p>
and the Hölder \(2\)-norm is called the Frobenius norm, which is also defined on infinite dimensional vector spaces as
</p>
\[
  \| X \|_F = \left(\Tr(XX^*)\right)^{1/2}
\]
<p>
where \(^*\) is the conjugate transpose.
</p>

<p>
<em>Def</em>: As before, we can take \(p \to \infty\) to get the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms-Hölder"></span><strong id="Hölder">Hölder</strong> \(\infty\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms--norm"></span><strong id="-norm">-norm</strong> given by
</p>
\[
  \| X\|_{H, \infty} = \max_{\substack{i = 1, \dots n \\ j = 1, \dots, n}} |x_{ij}|.
\]

<p>
<em>Def</em>: We can also define norms on matrices by viewing them as linear maps \(A: \mathbb R^n \to \mathbb R^m\); in particular, if we have some norm \(\| \cdot \|_a\) on \(\mathbb R^n\) and some norm \(\| \cdot \|_b\) on \(\mathbb R^m\), we may define the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms-operator norm"></span><strong id="operator norm">operator norm</strong> (or <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms-induced norm"></span><strong id="induced norm">induced norm</strong>)
</p>
\[
  \| A \|_{a, b} = \max_{x \neq 0} \frac{\| A x \|_b}{\| x \|_a}.
\]
<p>
In particular, if the norms on the domain and codomain are just \(p\)-norms, we write
</p>
\[
  \| A \|_{p} = \max_{x \neq 0}\frac{\| A x\|_p}{\| x \|_p}
\]
<p>
and call it the \(p\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms--norm"></span><strong id="-norm">-norm</strong> of \(A\). In particular, we call the \(2\)-norm the spectral norm. Further, the \(1\)-norm and \(\infty\)-norm are just
</p>
\[
  \| A \|_1 = \max_{j = 1, \dots, n} \left(\sum_{i=1}^m |a_{ij}| \right),
\]
<p>
which is the max column sum, and
</p>
\[
  \| A \|_\infty = \max_{i = 1, \dots, m} \left(\sum_{j=1}^n |a_{ij}| \right),
\]
<p>
which is the max row sum; both facts are easy to check.
</p>

<p>
In general, for \(p \notin \{1, 2, \infty\}\), computing \(\| A \|_p\) is NP-hard, and if we consider \(\| A \|_{p,q}\) then \(\|A\|_{\infty, 1}\) is hard and \(\|A\|_{1, \infty}\) is easy.
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms"><h4 id="Properties of Norms" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms">Properties of Norms</a></h4></div>

<p>
We may also want to consider some other desirable properties on our norms.
</p>
<ul>
<li>
For example, we might want <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms-submultiplicativity"></span><strong id="submultiplicativity">submultiplicativity</strong>:
\[
  \| A B\| \leq \|A \| \| B \|.
\]

</ul>
<p>
The Frobenius norm is submultiplicative.
</p>
<ul>
<li>
Take also <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms-consistency"></span><strong id="consistency">consistency</strong>:
\[
  \| Ax \|_b \leq \| A \|_{a,b} \|x\|_a.
\]

</ul>
<p>
This is true for \(p\)-norms, but not in general.
</p>

<p>
Some properties always hold.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms-Prop"></span><strong id="Prop">Prop</strong>: Every norm is Lipschitz.
</p>

<p>
<em>Proof</em>: Let our norm be \(\| \cdot \| : V \to \R\). The triangle inequality immediately implies
</p>
\[
  | \| u \| - \| v \| | \leq \| u - v \|.
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms-Theorem (Equivalence of Norms)"></span><strong id="Theorem (Equivalence of Norms)">Theorem (Equivalence of Norms)</strong>: <a href="https://kconrad.math.uconn.edu/blurbs/gradnumthy/equivnorms.pdf">Link</a> Set \(V\) a finite-dimensional vector space. Then every pair of norms \(\| \cdot \|_a, \| \cdot \|_b\) are equivalent to each other, e.g. there are constants \(c_1, c_2\) such that for any \(v \in V\),
</p>
\[
  c_1\| v \|_b \leq \| v \|_a \leq c_2 \| v \|_b.
\]

<p>
<em>Proof</em>: Induct on the dimension of \(V\) and see that every norm is equivalent to the infinity norm.
</p>

<p>
<em>Def</em>: We say that a sequence \(\{ x_k \}_{k=1}^\infty\) of vectors <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms-converges"></span><strong id="converges">converges</strong> to \(x\) if
</p>
\[
  \lim_{k \to \infty} \| x_k - x \| = 0.
\]

<p>
Then, the above clearly shows that convergence in one norm implies convergence in every norm.
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products"><h4 id="Inner, Outer, Matrix Products" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products">Inner, Outer, Matrix Products</a></h4></div>

<p>
<em>Def</em>: Set \(V\) a \(K\)-vector space (where \(K = \R, \C\)). An <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-inner product"></span><strong id="inner product">inner product</strong> is a binary operation \(\langle \cdot, \cdot \rangle: V \times V \to \R\) which satisfies that
</p>
<ul>
<li>
\(\left\langle v, v \right\rangle \geq 0\) for all \(v \in V\),

<li>
\(\left\langle v, v,\right\rangle = 0\) if and only if \(v = 0\),

<li>
\(\left\langle u, v \right\rangle = \overline{ \left\langle v, u\right\rangle }\) for all \(u, v \in V\),

<li>
\(\left\langle \alpha_1 u_1 + \alpha_2 u_2, v\right\rangle = \alpha \left\langle u_1, v \right\rangle + \alpha_2 \left\langle u_2, v\right\rangle\) for all \(u_1, u_2, v \in V, \alpha_1, \alpha_2 \in K\).

</ul>
<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Prop"></span><strong id="Prop">Prop</strong>: For an inner product \(\left\langle \cdot, \cdot \right\rangle\),
</p>
\[
  \| v \| = \sqrt{ \left\langle v, v \right\rangle }
\]
<p>
is a norm. Furthermore, an arbitrary norm \(\| \|\) is induced by an inner product if and only if it satisfies the parallelogram law
</p>
\[
  \norm{u + v}^2 + \norm{u - v}^2 = 2 \norm u ^ 2 + 2 \norm v ^2.
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Theorem (Cauchy-Schwarz)"></span><strong id="Theorem (Cauchy-Schwarz)">Theorem (Cauchy-Schwarz)</strong>: <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Link</a> Let \(\norm \cdot\) be induced by \(\left\langle \cdot, \cdot \right\rangle\). Then
</p>
\[
  \sqrt{\left\langle u, v \right\rangle} \leq \norm u \norm v.
\]


<p>
<em>Def</em>: The <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-standard Euclidean inner product"></span><strong id="standard Euclidean inner product">standard Euclidean inner product</strong> on \(\C^n\) is
</p>
\[
  \left\langle x, y\right\rangle = x^*y.
\]

<p>
<em>Def</em>: The <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Frobenius"></span><strong id="Frobenius">Frobenius</strong> norm on \(\C^{m \times n}\) is 
</p>
\[
  \left\langle X, Y\right\rangle = \sum_{i=1}^m \sum_{j=1}^n x_{ij} y_{ij} = \Tr(X^*Y).
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Theorem (Hölder Inequality)"></span><strong id="Theorem (Hölder Inequality)">Theorem (Hölder Inequality)</strong>: <a href="https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality">Link</a> For \(x, y \in \C^n\) and \(p^{-1} + q^{-1} = 1\), we have
</p>
\[
  |x^*y| \leq \norm x_p \norm y_q.
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Theorem (Bessel Inequality)"></span><strong id="Theorem (Bessel Inequality)">Theorem (Bessel Inequality)</strong>: <a href="https://en.wikipedia.org/wiki/Bessel%27s_inequality">Link</a> For \(x \in \C^n\) and an orthonormal basis \(e_1, \dots, e_n\), we have 
</p>
\[
  \sum_{k=1}^n | \left\langle x, e_k \right\rangle^2  \leq \norm x_2.
\]

<p>
<em>Def</em>: The <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-outer product"></span><strong id="outer product">outer product</strong> is a binary operator \(\C^m \times \C^n \to \C^{m \times n}\) taking 
</p>
\[
  (x, y) \mapsto xy^*.
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Prop"></span><strong id="Prop">Prop</strong>: \(A \in \R^{m \times n}\) is an outer product iff and only if it has rank 1.
</p>

<p>
<em>Def</em>: The matrix product is a binary operator \(\C^{m \times n} \times \C^{n \times p} \to \C^{M \times p}\). Set \(A = \bmat{\alpha_1 &amp; \alpha_2 &amp; \cdots &amp; \alpha_m}^T\) and \(B = \bmat{\beta_1 &amp; \beta_2 &amp; \cdots &amp; \beta_p}\). Then,
</p>
\[
  AB = \bmat{
    \alpha_1^T \beta_1 &amp; \cdots &amp; \alpha_1^T \beta_n \\
    \alpha_2^T \beta_1 &amp; \cdots &amp; \alpha_2^T \beta_n \\
    \vdots &amp; \ddots &amp; \vdots \\
    \alpha_m^T \beta_1 &amp; \cdots &amp; \alpha_m^T \beta_n 
  } 
  = \bmat{A\beta_1 &amp; A\beta_2 &amp; \cdots &amp; A\beta_n} 
  = \bmat{\alpha_1^TB \\ \alpha_2^TB \\ \vdots \\ \alpha_m^TB}.
\]
<p>
Alternatively, it is uniquely characterized as the matrix representing the composition of \(A\) and \(B\) as operators.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Prop"></span><strong id="Prop">Prop</strong>: Let \(D = \diag(d_1, \dots, d_n)\), the diagonal matrix with entries \(d_1, \dots, d_n\); then
</p>
\[
  AD = \bmat{d_1\alpha_1, \dots, d_n \alpha_n}.
\]
<p>
Simiar for the other direction of multiplication.
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors"><h3 id="Eigenvalues and Eigenvectors" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors">Eigenvalues and Eigenvectors</a></h3></div>

<p>
<em>Def</em>: For a complex matrix \(A\), an <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-eigenvalue"></span><strong id="eigenvalue">eigenvalue</strong> \(\lambda \in \C\) and <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-eigenvector"></span><strong id="eigenvector">eigenvector</strong> \(v \neq 0\) satisfy
</p>
\[
  Av = \lambda v.
\]

<p>
<em>Def</em>: Furthermore, an <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-eigenspace"></span><strong id="eigenspace">eigenspace</strong> is the span of all eigenvectors correspnding to a single eigenvalue, the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-spectrum"></span><strong id="spectrum">spectrum</strong> of \(A\), \(\Spec(A)\) is the set of all eigenvalues of \(A\), and the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-spectral radius"></span><strong id="spectral radius">spectral radius</strong> is \(\rho(A) = \max_{\lambda \in \Spec(A)} |\lambda| = |\lambda_{\text{max}}|\). Sometimes we will call this the top eigenvector/eigenvalue.
</p>

<p>
As a convention, usually we implcitly order eigenvalues, e.g. \(|\lambda_1| \geq |\lambda_2| \geq \cdots \geq \lambda_n\).
</p>

<p>
<em>Def</em>: More generally, if \(v\) is a eigenvector of \(A^T\), we say that \(v\) is a <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-left eigenvector"></span><strong id="left eigenvector">left eigenvector</strong> of \(A\) (and thus usual eigenvectors are <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-right eigenvectors"></span><strong id="right eigenvectors">right eigenvectors</strong>).
</p>

<p>
There are a few warnings about these things:
</p>
<ul>
<li>
real matrices may have complex eigenvectors;

<li>
we normally normalize eigenvectors to have unit length;

<li>
left and right eigenvectors are usually not the same.

</ul>
<p>
<em>Def</em>: A square matrix \(A \in \C^{n \times n}\) is diagonalizable if it is simiarly to a diagonal matrix.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Prop"></span><strong id="Prop">Prop</strong>: \(A\) is diagonalizable if and only if it has \(n\) linearly independent eigenvectors.
</p>

<p>
<em>Proof</em>: The eigenvectors form a basis by intertibility; change basis from the eigenvectors, scale by eigenvalues, and change basis back. In particular, let \(A \in \C^{n \times n}\) have eigenvalues \(\lambda_1, \dots, \lambda_n\), with corresponding eigenvectors \(v_1, \dots, v_n\); set \(\Lambda = \diag(\lambda_1, \dots, \lambda_n)\) and \(V = \bmat{v_1, \dots, v_n}\). Then
</p>
\[
  A = V \Lambda V^{-1}.
\]

<p>
<em>Def</em>: The above \(A = X \Lambda X^{-1}\) is called the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-eigenvalue decomposition"></span><strong id="eigenvalue decomposition">eigenvalue decomposition</strong>, or <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-EVD"></span><strong id="EVD">EVD</strong>.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Prop"></span><strong id="Prop">Prop</strong>: The diagonal entries in a triangular matrix are just the eigenvalues.
</p>

<p>
<em>Def</em>: A matrix \(A \in \C^{n \times n}\) is <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-normal"></span><strong id="normal">normal</strong> if it commutes with its adjoint \(A^*\) (the conjugate transpose), and <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-unitary"></span><strong id="unitary">unitary</strong> if \(AA^* = A^*A = I\).
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Prop"></span><strong id="Prop">Prop</strong>: \(A\) is unitary if and only if it's columns (or rows) are orthonormal.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Theorem"></span><strong id="Theorem">Theorem</strong>: \(A \in \C^{n \times n}\) is normal if and only if it is unitarily diagonalizable, e.g. if \(A = V\Lambda V^*\) with \(V\) unitary.
</p>

<p>
<em>Def</em>: \(A \in \C^{n \times n}\) is <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Hermitian"></span><strong id="Hermitian">Hermitian</strong> if \(A^* = A\).
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Theorem (Spectral Theorem)"></span><strong id="Theorem (Spectral Theorem)">Theorem (Spectral Theorem)</strong>: <a href="https://en.wikipedia.org/wiki/Spectral_theorem ">Link</a> \(A \in \C^{n \times n}\) is Hermetian if and only if \(A\) is unitarily diagonalizable with all real eigenvalues.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Corollary"></span><strong id="Corollary">Corollary</strong>: \(A \in \R^{n \times n}\) is symmetric if and only if it is orthgonally diagonalizable with all eigenvalues real.
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Jordan Canonical Form"><h4 id="Jordan Canonical Form" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Jordan Canonical Form">Jordan Canonical Form</a></h4></div>

<p>
<em>Def</em>: <a href="https://en.wikipedia.org/wiki/Jordan_normal_form">Link</a> Any matrix can be written in <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Jordan Canonical Form-Jordan canonical form"></span><strong id="Jordan canonical form">Jordan canonical form</strong>, e.g.
</p>
\[
  A = XJX^{-1}
\]
<p>
where \(J\) is nonzero only on the diagonal and the superdiagonal, which has values only in \(\{0, 1\}\), such as
</p>
\[
  J = \begin{bmatrix}
    \lambda_1 &amp; 1         &amp;           &amp;   &amp; &amp; &amp; \\
              &amp; \lambda_1 &amp; 1         &amp;   &amp; &amp; &amp; \\
              &amp;           &amp; \lambda_1 &amp; 0 &amp; &amp; &amp; \\
              &amp;           &amp; &amp; \lambda_2 &amp; 0 &amp; &amp; \\
              &amp;           &amp;         &amp;  &amp; \lambda_3 &amp; 1 &amp; \\
              &amp;           &amp;         &amp;  &amp; &amp; \lambda_3  &amp; \\
  \end{bmatrix}
\]
<p>
for example.
</p>

<p>
The way it's written above is not by coincidence: you can permute everything so that \(J\) is composed of Jordan blocks, which have the same entry all the way down the diagonal and have a superdiagonal of only ones, e.g.
</p>
\[
  J = \begin{bmatrix}
    J_1 &amp; &amp; &amp; \\
    &amp; J_2 &amp; &amp; \\
    &amp; &amp; \ddots &amp; \\
    &amp; &amp; &amp; J_k \\
  \end{bmatrix}
\]
<p>
where each \(J_i = \lambda I + N\), where \(N\) has all zero entries except on the superdiagonal, on which it is always one.
</p>

<p>
Unfortunately, the JCF is pretty useless in application.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Jordan Canonical Form-Theorem (Golub-Wilkinson)"></span><strong id="Theorem (Golub-Wilkinson)">Theorem (Golub-Wilkinson)</strong>: The Jordan canonical form cannot be computed in finite precision.
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Spectral Radius"><h4 id="Spectral Radius" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Spectral Radius">Spectral Radius</a></h4></div>

<p>
Let's return to the spectral radius,
</p>
\[
  \rho(A) = \max_{\lambda \in \Spec(A)} |\lambda|.
\]

<p>
Any nilpotent matrix shows that \(\rho\) is not a norm, but the following is true.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Spectral Radius-Prop"></span><strong id="Prop">Prop</strong>: If \(\| \cdot \|: \C^{m \times m} \to \R\) is any consistent matrix norm, then \(\rho(A) \leq \| A \|\).
</p>

<p>
<em>Proof</em>: Look at the norm of the image of a top eigenvector.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Spectral Radius-Theorem"></span><strong id="Theorem">Theorem</strong>: Given any \(A \in \C^{n \times n}\), any positive \(\epsilon\), there is a consistent norm (in fact, an operator norm) \(\| \cdot \|: \C^{n \times m} \to \R\) such that 
</p>
\[
  \| A \| \leq \rho(A) + \epsilon.
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Spectral Radius-Prop"></span><strong id="Prop">Prop</strong>: Given any matrix norm \(\| \cdot \|: \C^{n \times m} \to R\), we have
</p>
\[
  \rho(A) = \lim_{k \to \infty} \| A^k \|^{1/k}.
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Spectral Radius-Lemma"></span><strong id="Lemma">Lemma</strong>: \(\lim_{k \to \infty} A^k = 0\)  if and only if \(\rho(A) &lt; 1\).
</p>

<p>
<em>Proof</em>: \((\implies)\) Set \(\lambda\) to be a top eigenvalue of \(A\), and \(x\) a corresponding eigenvector. Then, \(A^k x = \lambda^k x\). Send \(k \to \infty\) and conclude.
</p>

<p>
\((\impliedby)\) By the above theorems, there is some operator norm \(\| \cdot \|\) such that \(\|A\| \leq \rho(A) + \epsilon &lt; 1\). Send \(k \to \infty\) and use the fact that operator norms imply \(\| A^k \| \leq \|A\|^k\) and win.
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Finding Eigenvalues"><h4 id="Finding Eigenvalues" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Finding Eigenvalues">Finding Eigenvalues</a></h4></div>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Finding Eigenvalues-Theorem (Gershgorin Circle Theorem)"></span><strong id="Theorem (Gershgorin Circle Theorem)">Theorem (Gershgorin Circle Theorem)</strong>: <a href="https://en.wikipedia.org/wiki/Gershgorin_circle_theorem">Wikipedia</a> Let \(A \in \C^{n \times n}\), with entries \(a_{ij}\) and for \(1 \leq i \leq n\) set \(r_i = \sum_{j\neq i} |a_{ij}|\). Then, every eigenvalue of \(A\) lies within the union of the Gershgorin discs
</p>
\[
  G_i = \left\{ z \mid z \in \C, |z - a_{ii}| \leq \sum_{j \neq i} |a_{ij}| \right\}
\]
<p>
and the number of eigenvalues in each connected component is equal to the number of Gershgorin disks that constitute that component.
</p>

<p>
<em>Proof</em>: If \(A \in \C^{n \times n}\) is strictly diagonally dominant, e.g.
</p>
\[
  |a_{ii}| &gt; \sum_{j \neq i}|a_{ij}|
\]
<p>
for all \(1 \leq i \leq n\), then \(A\) is invertible. To see this, take any \(x \in \ker(A)\) so that \(\sum_{j=1}^n a_{ij}x_i = 0\). But look at the index \(k\) that witnesses \(\| x \|_\infty\):
</p>
\[
  -a_{kk}x_k = \sum_{j \neq k} a_{kj}x_j \implies |a_{kk}||x_k| \leq \sum_{j \neq k} |a_{kj}||x_k|
\]
<p>
so \(x = 0\) and \(\ker(A)\) is trivial.
</p>

<p>
We proceed to prove that any \(z \notin \bigcup_{i=1}^n G_i\) cannot be an eigenvalue by showing that \(A - zI\) is invertible. But this is clear, since \(A - zI\) in that case is strictly diagonally dominant.
</p>

<p>
<em>Def</em>: <a href="https://en.wikipedia.org/wiki/Schur_decomposition">Wikipedia</a> The <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Finding Eigenvalues-Schur decomposition"></span><strong id="Schur decomposition">Schur decomposition</strong> of a matrix \(A\) is \(A = QUQ^*\) such that \(Q\) is unitary and \(U\), the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Eigenvalues and Eigenvectors-Finding Eigenvalues-Schur form"></span><strong id="Schur form">Schur form</strong> of \(A\) is upper triangular.
</p>

<p>
Note that you can read off the eigenvalues of \(A\) from the diagonal of the Schur form, which is numerically stable to compute.
</p>

<p>
As an aside, how one numerically finds the roots of a polynomial \(p(x) = \sum_{k=0}^m c_kx^k\) is by forming the companion matrix
</p>
\[
  A = \begin{bmatrix}
    0 &amp; \cdots &amp; 0 &amp; -c_0 \\
    1 &amp; \ddots &amp;0 &amp; -c_1 \\
    \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
    0 &amp; \cdots &amp; 1 &amp; -c_{d-1} 
  \end{bmatrix}.
\]

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Singular Value Decomposition"><h3 id="Singular Value Decomposition" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Singular Value Decomposition">Singular Value Decomposition</a></h3></div>

<p>
<em>Def</em>: <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Link</a> The <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Singular Value Decomposition-SVD decomposition"></span><strong id="SVD decomposition">SVD decomposition</strong> of a real (resp. complex) matrix \(A \in \C^{m \times n}\)) is
</p>
\[
  A = U \Sigma V^*
\]
<p>
where \(U, V\) are orthogonal (resp. unitary) and \(\Sigma\) is diagonal (on the shorter diagonal) with nonnegative entries; that is, if \(m &gt; n\), then
</p>
\[
  \Sigma = \bmat{\diag(\sigma_1, \dots, \sigma_n) \\ \mathbf 0} \in \R^{m \times n}_{\geq 0}
\]
<p>
and if \(m &lt; n\),
</p>
\[
  \Sigma = \bmat{\diag(\sigma_1, \dots, \sigma_n) &amp; \mathbf 0} \in \R^{m \times n}_{\geq 0}
\]

<p>
<em>Def</em>: We may arrange \(\sigma_{1} \geq \sigma_2 \geq \dots \geq \sigma_{\min(m, n)} \geq 0\); we then call top few singular values the principal singular values. The columns of \(U\) are left singular vectors, and the columns of \(V\)  are right singular vectors.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Singular Value Decomposition-Prop"></span><strong id="Prop">Prop</strong>: The largest \(r\) for which \(r &gt; 0\)  is the rank of \(A\).
</p>

<p>
<em>Def</em>: We sometimes use the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Singular Value Decomposition-reduced SVD"></span><strong id="reduced SVD">reduced SVD</strong>, which is the same as the normal SVD but we drop the extra rows of \(\Sigma\) to force it to be a square diagonal matrix in \(\C^{\min\{m, n\} \times \min\{m, n\}}\). Then,
</p>
\[
  A = \sum_{k=1}^r \sigma_k u_r v_r^*
\]
<p>
where \(u_r, v_r\) are the columns of \(U, V\) in the reduced SVD and \(r = \operatorname{rank}(A)\), or the number of nonzero singular values.
</p>

<p>
<em>Def</em>: In a similar vein, the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Singular Value Decomposition-condensed SVD"></span><strong id="condensed SVD">condensed SVD</strong> is attained by removing all of the nonzero rows/columns of \(\Sigma\), so \(\Sigma \in \C^{r \times r}\).
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Singular Value Decomposition-Theorem"></span><strong id="Theorem">Theorem</strong>: Given any \(A \in \C^{m \times n}\), there are \(U \in \C^{m \times m}\), \(V \in \C^{n \times n}\), and 
</p>
\[
  \Sigma = \diag(\sigma_1, \dots, \sigma_r, 0, \dots, 0) \in \R_{\geq 0}^{m \times n}
\]
<p>
such that \(U, V\) are unitary and \(A = U \Sigma V^*\).
</p>

<p>
<em>Proof</em>: Form the matrix
</p>
\[
  W = \bmat{0 &amp; A \\ A^* &amp; 0}
\]
<p>
which is Hermitian. The spectral theorem gives an EVD, and if you look carefully you get the compact SVD:
</p>
\[
  W = \bmat{ U_r &amp; U_r \\ V_r &amp; -V_r } \bmat{
  \sigma_1 &amp; &amp; &amp; &amp; &amp;  &amp;\\
   &amp; \sigma_2 &amp; &amp; &amp; &amp; &amp;\\
   &amp; &amp; \ddots &amp; &amp; &amp; &amp;\\
   &amp; &amp;  &amp; \sigma_r &amp; &amp; &amp;\\
   &amp; &amp;  &amp; &amp; -\sigma_1 &amp; &amp; \\
   &amp; &amp;  &amp; &amp; &amp; \ddots &amp; \\
   &amp; &amp;  &amp; &amp; &amp; &amp; -\sigma_r\\ }
  \bmat{ U_r &amp; U_r \\ V_r &amp; -V_r }^*.
\]

<p>
Singular values have a nice property: we have for any singular value \(\sigma\) and left singular vector \(u\) and right singular value \(v\), \(Av = \sigma u\) and \(A^*v = \sigma v\); thus \(u, v\) are eigenvalues of \(AA^*\) and \(A^*A\) respectively.
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Singular Value Decomposition-Applications"><h4 id="Applications" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Singular Value Decomposition-Applications">Applications</a></h4></div>

<p>
We can read off many important quantities/spaces from the SVD. Let \(A = U \Sigma V^*\) be a SVD, with only \(r\) nonzero singular values.
</p>
<ul>
<li>
The rank is the number of nonzero singular values.

<li>
The absolute value of the determinant is the product of the signular values.

<li>
The two norm is the maximum singular value.

<li>
Set \(\sigma = (\sigma_1, \dots, \sigma_n)\); then the Frobenius norm of \(A\) is \(\| \sigma \|_2\), and we call the general case of this the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Singular Value Decomposition-Applications-Schatten norm"></span><strong id="Schatten norm">Schatten norm</strong>, e.g. \(\| A \|_{S, p} = \| \sigma \|_p\); the case of \(p = 1\) is also called the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Singular Value Decomposition-Applications-nuclear norm"></span><strong id="nuclear norm">nuclear norm</strong>.

<li>
The Ky Fan \((p, k)\) norm for an integer \(1 \leq k \leq \infty\) is 
\[
  \| A \|_{S, p, k} = \left( \sum_{i=1}^p \sigma_i^p \right)^{1/p}.
\]

<li>
The kernel of \(A = U \Sigma V^*\) is spanned by \(v_{r+1}, \dots, v_n\).

<li>
The image of \(A\) is \(u_1, \dots, u_r\).

<li>
The kernel of \(A^T\) (this is the cokernel) is spanned by \(v_{1, \dots, v_r}\).

<li>
The image of \(A^T\) is spanned by \(u_{r+1}, \dots, u_m\).

</ul>
<p>
You can also solve fundamental problems. For example, if you consider the linear system \(Ax = b\), it might not have exactly one solution, so we can translate this into the least squares system \(\min_{x \in \R^n} \| Ax - b \|_2^2\); this still might not have a unique solution when \(A\) is singular, so we consider the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Singular Value Decomposition-Applications-minimum norm least squares"></span><strong id="minimum norm least squares">minimum norm least squares</strong> problem: \(\min \{ \| x \|_2 \mid x \in \argmin \| A_x - b \|_2 \}\).
</p>

<p>
SVD solves all of these simultaneously; set \(A = U \Sigma V^T\); this means that we want \(\Sigma V^T x = U^T b\); equivalently, we just need to solve \(\Sigma y = c\), where \(b = Uc, x = Vy\). But this is trivial since \(\Sigma\) is diagonal.
</p>

<p>
 We could define the pseudoinverse of \(A\) to be \(A^+\) to be the matrix \(A^+\) that sends any \(b\) to the minimum norm least squares solution of \(Ax = b\).
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Singular Value Decomposition-Applications-Prop"></span><strong id="Prop">Prop</strong>: \(A^+\) is just \(V \Sigma^{-1} U^*\), where by \(\Sigma^{-1}\) is just gotten by flipping all the nonzero diagonal elements.
</p>

<p>
<em>Proof</em>: Clear.
</p>

<p>
<em>Def</em>: In general, the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Singular Value Decomposition-Applications-pseudoinverse"></span><strong id="pseudoinverse">pseudoinverse</strong> or <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Singular Value Decomposition-Applications-Moore-Penrose"></span><strong id="Moore-Penrose">Moore-Penrose</strong> inverse of \(A \in \K^{m \times n}\) is a matrix \(X \in \K^{n \times m}\) that satisfies 
</p>
<ul>
<li>
\((AX)^{*} = AX\),

<li>
\((XA)^* = XA\),

<li>
\(AXA = A\),

<li>
\(XAX = X\).

</ul>
<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Singular Value Decomposition-Applications-Prop"></span><strong id="Prop">Prop</strong>: The above gives a unique matrix and is in fact the same as \(A^+\) from earlier.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Singular Value Decomposition-Applications-Prop"></span><strong id="Prop">Prop</strong>: There following statements are true.
</p>
<ul>
<li>
\(A^+A\) and \(AA^+\) are not necessarily \(I\).

<li>
If \(A\) has full column rank \(A^+ = (A^*A)^{-1}A\).

<li>
If \(A\) has full row rank, \(A^+ = A^*(AA^*)^{-1}\).

</ul>

    </div>
    <div class="footer">
      <p><small>Page created on 2023-10-11</small></p>
    </div>
</body>
</html>
