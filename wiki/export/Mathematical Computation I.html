<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/wiki.css">
  <title>Mathematical Computation I</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="content">
      
<div id="Mathematical Computation I: Matrix Computation Course"><h1 id="Mathematical Computation I: Matrix Computation Course" class="header"><a href="#Mathematical Computation I: Matrix Computation Course">Mathematical Computation I: Matrix Computation Course</a></h1></div>
<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023"><h2 id="UChicago STAT 30900, Autumn 2023" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023">UChicago STAT 30900, Autumn 2023</a></h2></div>

\[
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

\DeclareMathOperator{\Jac}{Jac}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\diag}{diag}

\let\temp\phi
\let\phi\varphi
\let\varphi\temp

\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\bra}[1]{\left[#1\right]}
\newcommand{\cbra}[1]{\left\{#1\right\}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\lrnorm}[1]{\left\|#1\right\|}

\newcommand{\mat}[1]{\begin{matrix}#1\end{matrix}}
\newcommand{\pmat}[1]{\pa{\mat{#1}}}
\newcommand{\bmat}[1]{\bra{\mat{#1}}}

\newcommand{\pfrac}[2]{\pa{\frac{#1}{#2}}}
\newcommand{\bfrac}[2]{\bra{\frac{#1}{#2}}}
\newcommand{\psfrac}[2]{\pa{\sfrac{#1}{#2}}}
\newcommand{\bsfrac}[2]{\bra{\sfrac{#1}{#2}}}
\]

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review"><h3 id="Linear Algebra Review" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review">Linear Algebra Review</a></h3></div>

<p>
By convention, vectors are column vectors. 
</p>

<p>
Set \(V\) a vector space (almost always real or complex).
</p>

<p>
<em>Def</em>: \(\| \cdot \| : V \to \mathbb R\) is a <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-norm"></span><strong id="norm">norm</strong> if it satisfies the following properties:
</p>
<ul>
<li>
\(\|v\| \geq 0\) for any \(v \in V\),

<li>
\(\|v\| = 0\) iff \(v = 0\),

<li>
\(\|\alpha v\| = |\alpha| \|v\|\) for \(\alpha \in \mathbb R\) and \(v \in V\),

<li>
\( \|v + w\| \leq \|v\| + \|w\|\) for any \(v, w \in V\).

</ul>
<p>
Now, since this is a computational class, we only care about specific norms, almost all of which we can quickly qrite down. 
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms"><h4 id="Vector Norms" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms">Vector Norms</a></h4></div>

<p>
Set \(V = \mathbb R^n\) or \(\mathbb C^n\) equivalently.
</p>

<p>
<em>Def</em>: The <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms-Minkowski"></span><strong id="Minkowski">Minkowski</strong> or \(p\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms--norm"></span><strong id="-norm">-norm</strong> is given by
</p>
\[
  \| x \|_p = \left(\sum_{i=1} x_i^p\right)^{1/p}
\]
<p>
and we call the \(2\)-norm the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms-Euclidean norm"></span><strong id="Euclidean norm">Euclidean norm</strong> and the \(1\)-norm the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms-Manhattan norm"></span><strong id="Manhattan norm">Manhattan norm</strong>.
</p>

<p>
<em>Def</em>: The \(\infty\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms--norm"></span><strong id="-norm">-norm</strong> is the limit of \(p\)-norms as \(p \to \infty\), and is given by
</p>
\[
  \| x \|_\infty = \max_{i = 1, \dots, n} |x_i| = \lim_{p \to \infty} \| x \|_p.
\]

<p>
<em>Def</em>: For a weight vector \(\underline w = \begin{bmatrix}w_1, \dots, w_n\end{bmatrix}^T \in \mathbb R^n\), with each \(w_i &gt; 0\), we have that the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms-weighted"></span><strong id="weighted">weighted</strong> \(p\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms--norm"></span><strong id="-norm">-norm</strong> is
</p>
\[
  \| v \|_{\underline w, p} = \left(\sum_{i=1} w_i x_i^p\right)^{1/p}.
\]

<p>
<em>Def</em>: In general, for any positive definite matrix \(A\) (that is, \(x^TAx &gt; 0\) for all \(x \neq 0\)), we may consider the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Vector Norms-Mahalanobis norm"></span><strong id="Mahalanobis norm">Mahalanobis norm</strong>
</p>
\[
  \| v \|_{A} = \left(x^T A x\right)^{1/2}.
\]

<p>
As convention, the "default" norm when a subscript is omitted is the Euclidean norm.
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms"><h4 id="Matrix Norms" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms">Matrix Norms</a></h4></div>

<p>
Set \(V = \mathbb R^{m \times n}\) or \(\mathbb C^{m \times n}\) equivalently.
</p>

<p>
<em>Def</em>: The <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms-Hölder"></span><strong id="Hölder">Hölder</strong> \(p\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms--norms"></span><strong id="-norms">-norms</strong> are given by
</p>
\[
  \| X \|_{H, p} = \left(\sum_{i=1}^m\sum_{j=1}^m |x_{ij}|^p \right)^{1/p},
\]
<p>
and the Hölder \(2\)-norm is called the Frobenius norm, which is also defined on infinite dimensional vector spaces as
</p>
\[
  \| X \|_F = \left(\Tr(XX^*)\right)^{1/2}
\]
<p>
where \(^*\) is the conjugate transpose.
</p>

<p>
<em>Def</em>: As before, we can take \(p \to \infty\) to get the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms-Hölder"></span><strong id="Hölder">Hölder</strong> \(\infty\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms--norm"></span><strong id="-norm">-norm</strong> given by
</p>
\[
  \| X\|_{H, \infty} = \max_{\substack{i = 1, \dots n \\ j = 1, \dots, n}} |x_{ij}|.
\]

<p>
<em>Def</em>: We can also define norms on matrices by viewing them as linear maps \(A: \mathbb R^n \to \mathbb R^m\); in particular, if we have some norm \(\| \cdot \|_a\) on \(\mathbb R^n\) and some norm \(\| \cdot \|_b\) on \(\mathbb R^m\), we may define the <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms-operator norm"></span><strong id="operator norm">operator norm</strong> (or <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms-induced norm"></span><strong id="induced norm">induced norm</strong>)
</p>
\[
  \| A \|_{a, b} = \max_{x \neq 0} \frac{\| A x \|_b}{\| x \|_a}.
\]
<p>
In particular, if the norms on the domain and codomain are just \(p\)-norms, we write
</p>
\[
  \| A \|_{p} = \max_{x \neq 0}\frac{\| A x\|_p}{\| x \|_p}
\]
<p>
and call it the \(p\)<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Matrix Norms--norm"></span><strong id="-norm">-norm</strong> of \(A\). In particular, we call the \(2\)-norm the spectral norm. Further, the \(1\)-norm and \(\infty\)-norm are just
</p>
\[
  \| A \|_1 = \max_{j = 1, \dots, n} \left(\sum_{i=1}^m |a_{ij}| \right),
\]
<p>
which is the max column sum, and
</p>
\[
  \| A \|_\infty = \max_{i = 1, \dots, m} \left(\sum_{j=1}^n |a_{ij}| \right),
\]
<p>
which is the max row sum; both facts are easy to check.
</p>

<p>
In general, for \(p \notin \{1, 2, \infty\}\), computing \(\| A \|_p\) is NP-hard, and if we consider \(\| A \|_{p,q}\) then \(\|A\|_{\infty, 1}\) is hard and \(\|A\|_{1, \infty}\) is easy.
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms"><h4 id="Properties of Norms" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms">Properties of Norms</a></h4></div>

<p>
We may also want to consider some other desirable properties on our norms.
</p>
<ul>
<li>
For example, we might want <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms-submultiplicativity"></span><strong id="submultiplicativity">submultiplicativity</strong>:
\[
  \| A B\| \leq \|A \| \| B \|.
\]

</ul>
<p>
The Frobenius norm is submultiplicative.
</p>
<ul>
<li>
Take also <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms-consistency"></span><strong id="consistency">consistency</strong>:
\[
  \| Ax \|_b \leq \| A \|_{a,b} \|x\|_a.
\]

</ul>
<p>
This is true for \(p\)-norms, but not in general.
</p>

<p>
Some properties always hold.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms-Prop"></span><strong id="Prop">Prop</strong>: Every norm is Lipschitz.
</p>

<p>
<em>Proof</em>: Let our norm be \(\| \cdot \| : V \to \R\). The triangle inequality immediately implies
</p>
\[
  | \| u \| - \| v \| | \leq \| u - v \|.
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms-Theorem (Equivalence of Norms)"></span><strong id="Theorem (Equivalence of Norms)">Theorem (Equivalence of Norms)</strong>: <a href="https://kconrad.math.uconn.edu/blurbs/gradnumthy/equivnorms.pdf">Link</a>. Set \(V\) a finite-dimensional vector space. Then every pair of norms \(\| \cdot \|_a, \| \cdot \|_b\) are equivalent to each other, e.g. there are constants \(c_1, c_2\) such that for any \(v \in V\),
</p>
\[
  c_1\| v \|_b \leq \| v \|_a \leq c_2 \| v \|_b.
\]

<p>
<em>Proof</em>: Induct on the dimension of \(V\) and see that every norm is equivalent to the infinity norm.
</p>

<p>
<em>Def</em>: We say that a sequence \(\{ x_k \}_{k=1}^\infty\) of vectors <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Properties of Norms-converges"></span><strong id="converges">converges</strong> to \(x\) if
</p>
\[
  \lim_{k \to \infty} \| x_k - x \| = 0.
\]

<p>
Then, the above clearly shows that convergence in one norm implies convergence in every norm.
</p>

<div id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products"><h4 id="Inner, Outer, Matrix Products" class="header"><a href="#Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products">Inner, Outer, Matrix Products</a></h4></div>

<p>
<em>Def</em>: Set \(V\) a \(K\)-vector space (where \(K = \R, \C\)). An <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-inner product"></span><strong id="inner product">inner product</strong> is a binary operation \(\langle \cdot, \cdot \rangle: V \times V \to \R\) which satisfies that
</p>
<ul>
<li>
\(\left\langle v, v \right\rangle \geq 0\) for all \(v \in V\),

<li>
\(\left\langle v, v,\right\rangle = 0\) if and only if \(v = 0\),

<li>
\(\left\langle u, v \right\rangle = \overline{ \left\langle v, u\right\rangle }\) for all \(u, v \in V\),

<li>
\(\left\langle \alpha_1 u_1 + \alpha_2 u_2, v\right\rangle = \alpha \left\langle u_1, v \right\rangle + \alpha_2 \left\langle u_2, v\right\rangle\) for all \(u_1, u_2, v \in V, \alpha_1, \alpha_2 \in K\).

</ul>
<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Prop"></span><strong id="Prop">Prop</strong>: For an inner product \(\left\langle \cdot, \cdot \right\rangle\),
</p>
\[
  \| v \| = \sqrt{ \left\langle v, v \right\rangle }
\]
<p>
is a norm. Furthermore, an arbitrary norm \(\| \|\) is induced by an inner product if and only if it satisfies the parallelogram law
</p>
\[
  \norm{u + v}^2 + \norm{u - v}^2 = 2 \norm u ^ 2 + 2 \norm v ^2.
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Theorem (Cauchy-Schwarz)"></span><strong id="Theorem (Cauchy-Schwarz)">Theorem (Cauchy-Schwarz)</strong>: <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Link</a>. Let \(\norm \cdot\) be induced by \(\left\langle \cdot, \cdot \right\rangle\). Then
</p>
\[
  \sqrt{\left\langle u, v \right\rangle} \leq \norm u \norm v.
\]


<p>
<em>Def</em>: The <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-standard Euclidean inner product"></span><strong id="standard Euclidean inner product">standard Euclidean inner product</strong> on \(\C^n\) is
</p>
\[
  \left\langle x, y\right\rangle = x^*y.
\]

<p>
<em>Def</em>: The <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Frobenius"></span><strong id="Frobenius">Frobenius</strong> norm on \(\C^{m \times n}\) is 
</p>
\[
  \left\langle X, Y\right\rangle = \sum_{i=1}^m \sum_{j=1}^n x_{ij} y_{ij} = \Tr(X^*Y).
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Theorem (Hölder Inequality)"></span><strong id="Theorem (Hölder Inequality)">Theorem (Hölder Inequality)</strong>: <a href="https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality">Link</a>. For \(x, y \in \C^n\) and \(p^{-1} + q^{-1} = 1\), we have
</p>
\[
  |x^*y| \leq \norm x_p \norm y_q.
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Theorem (Bessel Inequality)"></span><strong id="Theorem (Bessel Inequality)">Theorem (Bessel Inequality)</strong>: <a href="https://en.wikipedia.org/wiki/Bessel%27s_inequality">Link</a>. For \(x \in \C^n\) and an orthonormal basis \(e_1, \dots, e_n\), we have 
</p>
\[
  \sum_{k=1}^n | \left\langle x, e_k \right\rangle^2  \leq \norm x_2.
\]

<p>
<em>Def</em>: The <span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-outer product"></span><strong id="outer product">outer product</strong> is a binary operator \(\C^m \times \C^n \to \C^{m \times n}\) taking 
</p>
\[
  (x, y) \mapsto xy^*.
\]

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-Prop"></span><strong id="Prop">Prop</strong>: \(A \in \R^{m \times n}\) is an outer product iff and only if it has rank 1.
</p>

<p>
<em>Def</em>: The matrix product is a binary operator \(\C^{m \times n} \times \C^{n \times p} \to \C^{M \times p}\). Set \(A = \bmat{\alpha_1 &amp; \alpha_2 &amp; \cdots &amp; \alpha_m}^T\) and \(B = \bmat{\beta_1 &amp; \beta_2 &amp; \cdots &amp; \beta_p}\). Then,
</p>
\[
  AB = \bmat{
    \alpha_1^T \beta_1 &amp; \cdots &amp; \alpha_1^T \beta_n \\
    \alpha_2^T \beta_1 &amp; \cdots &amp; \alpha_2^T \beta_n \\
    \vdots &amp; \ddots &amp; \vdots \\
    \alpha_m^T \beta_1 &amp; \cdots &amp; \alpha_m^T \beta_n 
  } 
  = \bmat{A\beta_1 &amp; A\beta_2 &amp; \cdots &amp; A\beta_n} 
  = \bmat{\alpha_1^TB \\ \alpha_2^TB \\ \vdots \\ \alpha_m^TB}.
\]
<p>
Alternatively, it is uniquely characterized as the matrix representing the composition of \(A\) and \(B\) as operators.
</p>

<p>
<span id="Mathematical Computation I: Matrix Computation Course-UChicago STAT 30900, Autumn 2023-Linear Algebra Review-Inner, Outer, Matrix Products-<em>Prop</em>"></span><strong id="<em>Prop</em>"><em>Prop</em></strong>: Let \(D = \diag(d_1, \dots, d_n)\), the diagonal matrix with entries \(d_1, \dots, d_n\); then
</p>
\[
  AD = \bmat{d_1\alpha_1, \dots, d_n \alpha_n}.
\]
<p>
Simiar for the other direction of multiplication.
</p>

    </div>
    <div class="footer">
      <p><small>Page created on 2023-10-02</small></p>
    </div>
</body>
</html>
